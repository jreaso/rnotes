[["index.html", "R Notes Resources", " R Notes Jamie Reason 2022-23 Resources These are personal notes on R mainly for reference, written over the course of two modules taken at university, MLLN (Machine Learning and Neural Networks) (2022-23) DSSC (Data Science and Statistical Computing) (2021-22) Although, this current version is more focused on the content for the MLLN module. These are heavily based off the notes of the respective courses (see links below) and I do not take credit. There may still be mistakes and inaccuracies from myself. R Seek Search Engine Cheat Sheets MLNN Course Notes DSSC Course Notes "],["r-basics.html", "Chapter 1 R Basics 1.1 Vectors 1.2 Data Frames 1.3 Lists 1.4 Data Types 1.5 Other", " Chapter 1 R Basics 1.1 Vectors 1.1.1 Creating Vectors # basic vector v &lt;- c(2,3,8,5) #repeated values rep(c(1,2),5) #sequence seq(4,7) #inclusive, and by default counts by 1 #shorthand 4:7 #custom length or increments seq(0, 10, by = 2) seq(0, 3, length.out = 7) 1.1.2 Accessing Vectors myData &lt;- c(2,3,8,5) #element wise myData[2] #selects second entry myData[-3] #excludes third element #can also pass in vectors to index it myData[c(1,4)] myData[2:4] 1.1.3 Vector Functions Essential Functions mean() sd() var() max() min() median() range() quantile() cumsum() sum() Other Functions sort() - sorts a vector (alphabetically or by increasing size when numerical) rank() - provides the rank of each element order() - gives the indices of the elements in order unique() - returns just the unique values in the vector length() - total number of elements in the vector paste() - makes each element in the vector a string Particularly Interesting Functions sample() - randomly sample from the elements of a vector sample(c(3,7,9,23,45), 3, replace = FALSE) #sample from vector, choosing n=3 without replacement table() - provide counts of the occurrence of each element table(sample(1:6, 200, replace = TRUE)) is.na() - gives a TRUE/FALSE vector as the output checking is an entry is NA 1.2 Data Frames Structure and viewing data frames To show a structure of a variable, dataframe, list etc, run str(x). View(mydata) to see in spreadsheet view 1.2.1 Creating Data Frames Manually hw &lt;- data.frame(Height = c(147, 150, 152), Weight = c(52.2, 53.1, 54.4)) Load from *.csv file If in working directory, hw &lt;- read.csv(&quot;hw.csv&quot;) Can also use RStudio “Import Dataset” button in Environment tab (top left) Built in Data Sets Can use data frames built into R, for example: data(&quot;cars&quot;) Run data() to see all data sets 1.2.2 Accessing Data Frames Single column (alternative ways): will collapse Data Frame column into vector hw$Height hw[,1] keeps data frame structure only keeping 1 column hw[,1, drop = FALSE] Single element: hw$Height[3] or hw[3,1] Single row: hw[3,] (similar to single column) Full access df[ r , c ] r can be empty, integer between 1 and number of row, vector of integers or a vector of TRUE/FALSE of length num rows (must be exact length). c can be empty, integer between 1 and number of row, vector of integers, a vector of TRUE/FALSE of length num rows (must be exact length) or vector of strings with column/variable names. Examples (first 3 equivalent) wq.red$ph wq.red[, 9] wq.red[, &quot;pH&quot;] wq.red[, c(9,11)] wq.red[, c(&quot;pH&quot;,&quot;alcohol&quot;)] 1.2.2.1 Advanced Query - Accessing a Subset of a Data Frame Examples Entire column “Weight” with values &gt;50 hw[hw$Weight&gt;50,] Both columns, 4 random rows (using sample()) hw[sample(1:nrow(hw), 4),] -Removing NA values movies[!is.na(movies$budget),] #doesn&#39;t have any NA value Note: ! is the logical NOT operator and is.na() is a function that acts on a vector giving a TRUE/FALSE vector as the output extra example wq.red[wq.red$pH&gt;3 &amp; wq.red$density&lt;1, &quot;alcohol&quot;] #wq.red$pH&gt;3 is a vector of TRUE/FALSE #wq.red$density&lt;3 is a vector of TRUE/FALSE #&amp; takes logical between two vectors 1.2.3 Interrogating Data Frames Data Frame Functions - information names() - column names dim() - number of rows and columns nrow() - number of rows ncol() - number of columns head() - useful for large data, just shows top rows (can add extra parameter to specify how many rows show up) str() - shows details about type of data Data Frame Functions - interesting data colMeans() rowMeans() colSums() rowSums() cov() - covariance matrix cor() - correlation matrix scale() - scales data to be centered at 0 and scaled (both have optional arguments available) summary() - gives all major statistics for each variable (column) Data Frame (column/row) Functions - sorting and ordering (optional decreasing = TRUE argument) `sort(my.data$var) - sorts a variable but only outputs that column vector sorted `order(my.data$var) - outputs a list of indices sorted Sorting a Data Frame by a Column my.data[order(my.data$var),] - sorts whole data frame according to var column 1.2.4 Manipulating Data Frames Creating/Adding Variables to a Data Frame Reference a variable that doesn’t exist and just assign it to something. For example, hw$BMI &lt;- hw$Weight/(hw$Height/100)^2 1.2.4.1 Merging Data Frames rbind(,) and cbind(,) Note: can be very error prone and often better to use tidyverse (unless with rbind variables are identical and in same order or with cbind, observations are in same order) rbind() pastes rows together (above/below) cbind() pastes columns together (left/right) For an example with rbind(,), if two data frames have same column names, rbind(,) will stack the rows to make one data frame. As an example, test1 &lt;- data.frame(Col1 = c(1,5,9,2), Col2 = c(6,9,8,3)) test2 &lt;- data.frame(Col2 = c(5,9,0,1), Col1 = c(4,9,3,0)) rbind(test1, test2) ## Col1 Col2 ## 1 1 6 ## 2 5 9 ## 3 9 8 ## 4 2 3 ## 5 4 5 ## 6 9 9 ## 7 3 0 ## 8 0 1 (notice how the columns were matched by name not order) 1.2.5 Missing Data in Data Frames 1.2.5.1 Importing Data with Missing Values When using read.csv(\"mydata.csv\"), can add the additional argument na.strings=c(...) to set any strings in the vector to be replaced by &lt;NA&gt;. As an example, read.csv(&quot;carsdata.csv&quot;, na.strings = c(&quot;&quot;, &quot;na&quot;)) would import the cardata.csv file as a data frame with all strings that are empty \"\" or \"na\" with the approptiate &lt;NA&gt; tag. Ignoring NA values - na.rm = TRUE na.rm = TRUE argument ignores all &lt;NA&gt; values when performing the function. As an example, mean(mydata$var, na.rm = TRUE) which is equivalent to mean(na.omit(mydata$var)) 1.2.5.2 Remove Ovservations with &lt;NA&gt; Entry (na.omit()) Example, na.omit(chickwts$weight) 1.3 Lists x &lt;- list(1, &quot;a&quot;, c(1,2,3), data.frame(a = 1:3, b = 4:6)) can have a list of any type of variable can be good for hierarchical and tree structures, Nesting is permitted (i.e. lists can contain lists) variables in the list can also be named x &lt;- list(bob = 1, jill = &quot;a&quot;, jack = c(1,2,3), eve = data.frame(a = 1:3, b = 4:6)) x$eve$a ## [1] 1 2 3 1.3.1 Accessing lists x[] #accesses an element of a list (returns a list of one element) x[[]] #strips away one level of hierarchy, list structure is gone x$bob #equivalent to above when items in list are named 1.4 Data Types Numeric Logical (TRUE/FALSE) Categorical (called factors in R), could be ordered (e.g. credit rating) or could not be (e.g. eye colour) Date/Time Text or String Others (e.g. image, spatial, audio, video) 1.4.1 Categorical Data (Factors) 1.4.1.1 Making Data Frames Categorical R cannot tell the difference between factors and strings when importing data frames, so we must “tidy” them up after importing Cleaning up a data frame (all non-numeric by default get imported as strings) by making all strings in one variable a factor eyesDF &lt;- data.frame(name=c(&quot;anne&quot;,&quot;john&quot;,&quot;charlie&quot;,&quot;sarah&quot;,&quot;max&quot;,&quot;ellie&quot;,&quot;eve&quot;), eyeColour=c(&quot;blue&quot;,&quot;green&quot;,&quot;brown&quot;,&quot;brown&quot;,&quot;blue&quot;,&quot;blue&quot;,&quot;brown&quot;)) #changes a variable to be categorical (a factor) eyesDF$eyeColour &lt;- as.factor(eyesDF$eyeColour) summary(eyesDF$eyeColour) ## blue brown green ## 3 3 1 Treat data as a factor or a text? Treat as text not factor when every observation is unique (e.g. surname) When some text is coming up very often it may be more appropriate as a factor 1.4.1.2 Making Vectors Categorical We can create a vector with factors in by creating the entries as strings and then applying the factor() function For example, eye.colour &lt;- factor(c(&quot;blue&quot;,&quot;brown&quot;,&quot;green&quot;,&quot;blue&quot;,&quot;blue&quot;,&quot;brown&quot;,&quot;green&quot;,&quot;blue&quot;,&quot;blue&quot;,&quot;green&quot;,&quot;blue&quot;,&quot;green&quot;,&quot;blue&quot;,&quot;blue&quot;,&quot;brown&quot;,&quot;green&quot;,&quot;brown&quot;,&quot;brown&quot;,&quot;green&quot;)) summary(eye.colour) ## blue brown green ## 8 5 6 Example Using Factors The following data frame has a factor variable (feed) data(&quot;chickwts&quot;) head(chickwts) ## weight feed ## 1 179 horsebean ## 2 160 horsebean ## 3 136 horsebean ## 4 227 horsebean ## 5 217 horsebean ## 6 168 horsebean summary(chickwts) ## weight feed ## Min. :108.0 casein :12 ## 1st Qu.:204.5 horsebean:10 ## Median :258.0 linseed :12 ## Mean :261.3 meatmeal :11 ## 3rd Qu.:323.5 soybean :14 ## Max. :423.0 sunflower:12 We can filter out specific factors, chickwts[chickwts$feed %in% c(&quot;sunflower&quot;,&quot;linseed&quot;),] Number of levels (different factors) and names of levels nlevels(chickwts$feed) ## [1] 6 levels(chickwts$feed) ## [1] &quot;casein&quot; &quot;horsebean&quot; &quot;linseed&quot; &quot;meatmeal&quot; &quot;soybean&quot; &quot;sunflower&quot; unclass() gives each entry a number corresponding to a factor unclass(chickwts$feed) ## [1] 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3 3 5 5 5 5 5 5 5 5 5 5 5 5 5 5 6 6 ## [39] 6 6 6 6 6 6 6 6 6 6 4 4 4 4 4 4 4 4 4 4 4 1 1 1 1 1 1 1 1 1 1 1 1 ## attr(,&quot;levels&quot;) ## [1] &quot;casein&quot; &quot;horsebean&quot; &quot;linseed&quot; &quot;meatmeal&quot; &quot;soybean&quot; &quot;sunflower&quot; 1.5 Other Packages Installing a package: install.packages(\"dplyr\") Loading a package: library(\"dplyr\") Loops for(i in x) { #i iterates over the values in vector x } Functions myFunction &lt;- function(arg1, arg2 = 1) { ... return(...) } Functions acting on vectors - sapply() sapply(X, Fun) Applies function Fun to vector X element-wise Logical Operators Logical operators act element-wise on pairs of vectors (of same size) of TRUE and FALSE values. AND is &amp;, OR is |. "],["plotting.html", "Chapter 2 Plotting 2.1 Plotting in Base R 2.2 Plotting in ggplot2", " Chapter 2 Plotting 2.1 Plotting in Base R (useful for exploration of data) 2.1.1 Core Plot Function Plots data x against data y. (if only x is supplied, the indices will be plotted against their values) plot(x,y, ...) Optional Arguments col - colour of points (can use RGB or colour name as a string; can be vector for each point) pch - plotting symbol (cross, circle etc), an integer Plotting symbol reference xlab and ylab - labels xlim and ylim - limits in the form of a 2-vector (e.g. xlim = c(20,100) restricts x from 20 to 100) main - Plot Title type \"p\" - points (default) \"l\" - line connecting observations \"b\" - both points and lines 2.1.2 Other Plot Functions hist() - Histogram boxplot() - Boxplot barplot() - Categorical Bar Charts (use table to get summary) Note: you can store plots inside of variables data(&quot;diamonds&quot;, package = &quot;ggplot2&quot;) hgram &lt;- hist(diamonds$price, freq = FALSE) str(hgram) ## List of 6 ## $ breaks : num [1:20] 0 1000 2000 3000 4000 5000 6000 7000 8000 9000 ... ## $ counts : int [1:19] 14524 9683 6129 4225 4665 3163 2278 1668 1307 1076 ... ## $ density : num [1:19] 2.69e-04 1.80e-04 1.14e-04 7.83e-05 8.65e-05 ... ## $ mids : num [1:19] 500 1500 2500 3500 4500 5500 6500 7500 8500 9500 ... ## $ xname : chr &quot;diamonds$price&quot; ## $ equidist: logi TRUE ## - attr(*, &quot;class&quot;)= chr &quot;histogram&quot; 2.1.3 Adding to Plots Each plot() function creates a new plot. To add to an existing plot use, points() - adds a plot of points to an existing plot lines() - shorthand for points(x, y, type=\"l\") abline() - adds a \\(y=mx+c\\) line directly 2.1.3.1 Fitting Lines to Plots (see linear regression) lm() - fits a straight line, pass inside of abline() lowess() - fits a smooth line, pass inside of lines() (f argument controls smoothness) density() - fits a smooth continuous version of a histogram Example data(&quot;diamonds&quot;, package = &quot;ggplot2&quot;) plot(diamonds$carat, diamonds$price, pch = 20) abline(lm(price ~ carat, diamonds), col = &quot;red&quot;) lines(lowess(diamonds$carat, diamonds$price, f = 0.05), col = &quot;green&quot;) hist(diamonds$price, freq = FALSE) lines(density(diamonds$price), col = &quot;red&quot;) 2.1.4 Multiple Plots (often better to just use ggplot2) To get a grid of all pairwise scatter plots, use pairs() pairs(mtcars) pairs(mtcars[,1:4]) You can also manually set the grid size (using par(mfrow = c(n,m))) and then populate each grid slot one by one by calling slots par(mfrow = c(2,1)) plot(diamonds$carat, diamonds$price) boxplot(diamonds$carat) par(mfrow = c(1,1)) # &lt;- need this to reset to a single plot! To reset the plotting window to default use dev.off(). 2.2 Plotting in ggplot2 (useful for presentation of data) Loading ggplot2, # Either ... library(&quot;tidyverse&quot;) # for all tidyverse packages # OR, for just plotting library(&quot;ggplot2&quot;) 2.2.1 Main Structure Starting a plot Every plot starts with the function ggplot() with the optional arguments: data - to specify the data frame containing the variables we later reference aes() - mapping to specify what variables map to the x axis, y axis, colour legend, etc For example, ## Warning: package &#39;tidyverse&#39; was built under R version 4.1.2 ## Warning: package &#39;ggplot2&#39; was built under R version 4.1.2 ## Warning: package &#39;tibble&#39; was built under R version 4.1.2 ## Warning: package &#39;tidyr&#39; was built under R version 4.1.2 ## Warning: package &#39;readr&#39; was built under R version 4.1.2 ## Warning: package &#39;purrr&#39; was built under R version 4.1.2 ## Warning: package &#39;dplyr&#39; was built under R version 4.1.2 ## Warning: package &#39;stringr&#39; was built under R version 4.1.2 ## Warning: package &#39;forcats&#39; was built under R version 4.1.2 ggplot(diamonds, aes(x = carat, y = price)) Axis are labelled and scaled but nothing is plotted yet (as we have not called a “Geom”). Geoms A geom_ will add a layer to the plot. Examples of Geoms: geom_point() - most basic, plots x against y as scatter plot geom_line() geom_smooth() - smoothed curve (defaukt method is “gam”, can also use “lm”) geom_bar() - barchart (1 variable and counts) geom_col() - barchart (2 variables) geom_boxplot() - boxplot More unusual ones, geom_hex() geom_polygon() aes() If you want to specify the x and y variables, colour by a property, group by a property, change the point size based on a property etc then you put that information into the aes(). The aes(...) that goes into the original ggplot(aes()) will be inherited by all plots unless overridden. The aes(...) that goes into a particular geom, geom_...(aes()) only applies to that geom. Labels xlab(\"X-axis Label\"), ylab(\"Y-axis Label\") and ggtitle(\"Title\") can also be added to the plot in the same way as Geoms. Alternativily, use + labs(title=\"Title\", x=\"X-axis\", y=\"Y-axis\") 2.2.2 Updating a Plot (Plots in Variables) data(&quot;mtcars&quot;) p &lt;- ggplot(mtcars, aes(x = hp, y = mpg)) + geom_point() p + geom_smooth() p + geom_smooth(method = &quot;lm&quot;) p + scale_y_log10() + scale_x_log10() + geom_smooth(method = &quot;lm&quot;) p + scale_y_log10() + scale_x_log10() + geom_smooth(method = &quot;lm&quot;) + geom_vline(xintercept = 100) Here, p stores the basic plot and each time, something different is added to it for a new plot, but without updating p. 2.2.3 Faceting Faceting enables splitting your data into multiple plots according to a categorical variable. facet_wrap() - a single variable split formula notation to indicate splitting variable ~ var optionally specify number of rows facet_grid() - two variable split formula indicating both splitting variables rows_var ~ cols_var formula indicating both splitting variables rows_var ~ cols_var For example, ggplot(mtcars, aes(x = hp, y = mpg)) + facet_wrap(~ gear) + geom_point() ggplot(mtcars, aes(x = hp, y = mpg)) + facet_grid(cyl ~ gear) + geom_point() 2.2.4 Examples ggplot(diamonds, aes(x = carat, y = price)) + geom_point(aes(colour = cut), size = 0.2) + geom_smooth(aes(colour = cut)) + xlab(&quot;Number of carats&quot;) + ylab(&quot;Price in $&quot;) ggplot(mpg, aes(x=displ, y=hwy)) + geom_point(aes(colour = class)) ggplot(mpg, aes(x=displ, y=hwy)) + facet_wrap(~class) + geom_point() + geom_point(aes(y=cty), colour=&quot;red&quot;) + #aes() doesn&#39;t hold all information!!! ylab(&quot;Fuel efficiency&quot;) ggplot(mpg, aes(x=displ, y=hwy)) + geom_point(aes(colour=drv)) + geom_smooth(colour=&quot;black&quot;) + geom_smooth(aes(colour=drv)) ggplot(mpg, aes(x=class)) + geom_bar(aes(fill=drv)) "],["simple-linear-regression-models.html", "Chapter 3 Simple Linear Regression Models 3.1 Building an SLR Model 3.2 Plotting an SLR Model 3.3 Diagnostic Plots and Residual Analysis 3.4 Transforming Regression Variables 3.5 Confidence and Prediction Intervals 3.6 Misc", " Chapter 3 Simple Linear Regression Models 3.1 Building an SLR Model lm stands for Linear Model and is the function used for Linear Regression model &lt;- lm(Y ~ X, data) Practical 1 Example data(faithful) model &lt;- lm(waiting ~ eruptions, faithful) summary(model) ## ## Call: ## lm(formula = waiting ~ eruptions, data = faithful) ## ## Residuals: ## Min 1Q Median 3Q Max ## -12.0796 -4.4831 0.2122 3.9246 15.9719 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 33.4744 1.1549 28.98 &lt;2e-16 *** ## eruptions 10.7296 0.3148 34.09 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 5.914 on 270 degrees of freedom ## Multiple R-squared: 0.8115, Adjusted R-squared: 0.8108 ## F-statistic: 1162 on 1 and 270 DF, p-value: &lt; 2.2e-16 Useful functions to extract data from model: summ &lt;- summary(model) coef(model) gives coefficients fitted(model) returns the vector of the fitted values, \\(\\hat{y}_i = b_0 + b_1 x_i\\) resid(model) (or summ$residuals) returns vector of residuals, \\(e_i = y_i - \\hat{y}_i\\) summ$coefficients gives more information on coefficient estimates (standard error, t-statistic, corresponding two-sided p-value) summ$sigma extracts regression standard error summ$r.squared returns value of \\(R^2\\) 3.2 Plotting an SLR Model Using Base R plot(faithful$waiting ~ faithful$eruptions, xlab=&quot;Eruption Time (m)&quot;, ylab=&quot;Waiting Time Between Eruptions (m)&quot;, pch=16, col=&quot;cornflowerblue&quot;) abline(model, col=&quot;red&quot;) Using ggplot ggplot(faithful, aes(x=eruptions, y=waiting)) + geom_point() + geom_smooth(method=lm, formula = y~x, se=FALSE) geom_smooth Documentaion 3.3 Diagnostic Plots and Residual Analysis Infant Mortality and GDP Example from MLLN Notes Section 3.3 model1 &lt;- fit&lt;- lm(infantMortality ~ ppgdp, data=newUN) plot(newUN$infantMortality ~ newUN$ppgdp, xlab=&quot;GDP per Capita&quot;, ylab=&quot;Infant mortality (per 1000 births)&quot;, pch=16, col=&quot;cornflowerblue&quot;, main=&quot;Model 1&quot;) abline(model1,col=&quot;red&quot;) model2 &lt;- lm(log(infantMortality) ~ log(ppgdp), data=newUN) plot(log(newUN$infantMortality) ~ log(newUN$ppgdp), pch=16, col=&quot;cornflowerblue&quot;, main=&quot;Model 2&quot;) abline(model2,col=&quot;red&quot;) model1 clearly doesn’t fit SLR, we can confirm this be looking at diagnostic plots model2 which is a transformation fits better and is an example of good diagnostic plots Residual Plot plot(model1, which=1, pch=16, col=&quot;cornflowerblue&quot;, main=&quot;Model 1 Residual Plot&quot;) Comparison for Model 1 (poor fit) and Model 2 (good fit) par(mfrow=c(1,2)) plot(model1, which=1, pch=16, col=&quot;cornflowerblue&quot;, main=&quot;Model 1 Residual Plot&quot;) plot(model2,which=1,pch=16,col=&quot;cornflowerblue&quot;, main=&quot;Model 2 Residual Plot&quot;) Example Using Simpler Methods (From Practical 1) model is as from Practical 1 par(mfrow=c(1,2)) plot(y = resid(model), x=fitted(model)) #residuals against fitted values plot(y = resid(model), x=d) #residuals against raw values Residual Q-Q Plot and Histogram Residual Q-Q Plot plot(model1, which=2, pch=16, col=&quot;cornflowerblue&quot;, main=&quot;Model 1 Q-Q Plot&quot;) Residual Histogram hist(resid(model1), col=&quot;cornflowerblue&quot;, main=&quot;Model 1 Residual Histogram&quot;) Comparison for Model 1 (poor fit) and Model 2 (good fit) par(mfrow=c(2,2)) # Model 1 (before transformation) plot(model1, which = 2,pch=16, col=&quot;cornflowerblue&quot;, main=&quot;Model 1 Q-Q&quot;) hist(resid(model1),col=&quot;cornflowerblue&quot;, main=&quot;Model 1 Resid Hist&quot;) # Model 2 (after transformation) plot(model2, which = 2, pch=16, col=&quot;hotpink3&quot;, main=&quot;Model 2 Q-Q&quot;) hist(resid(model2),col=&quot;hotpink3&quot;, main=&quot;Model 2 Resid Hist&quot;) Example Using Simpler Methods (From Practical 1) model is as from Practical 1 par(mfrow=c(1,2)) hist(resid(model)) qqnorm(resid(model)) 3.4 Transforming Regression Variables When a linear regression model doesn’t look like a good fit, it may be appropriate to transform one or both of the variables. Example Making Log transformation (for Y and X) transformed_model &lt;- lm(log(Y) ~ log(X), data) Example Making Polynomial Transformation transformed_model &lt;- lm(Y ~ I(X^2), data) # I() is a general wrapper Using ggplot Change the formula parameter in geom_smooth 3.5 Confidence and Prediction Intervals 3.5.1 Confidence Intervals Easiest to use confint(). confint(model, level=...) gives a confidence interval for each coefficient. Example from Notes carSales&lt;-data.frame(Price=c(85,103,70,82,89,98,66,95,169,70,48), Age=c(5,4,6,5,5,5,6,6,2,7,7)) reg &lt;- lm(Price ~ Age, carSales) confint(reg, level=0.95) #CI for parameters ## 2.5 % 97.5 % ## (Intercept) 160.99243 229.94451 ## Age -26.59419 -13.92833 Practical 1 Example Again, model is as from Practical 1 beta1hat &lt;- coef(model)[2] se.beta1 &lt;- summary(model)$coefficients[2,2] n &lt;- length(w) #The Confidence Interval is beta1hat + c(-1,1) * qt(0.975, n-2) * se.beta1 ## [1] 10.10996 11.34932 # or confint(model, level=0.95)[2,] ## 2.5 % 97.5 % ## 10.10996 11.34932 CI at a new point When newpoints is a data frame with same x column name and a new point you can calculate the model CI around that point using predict() with interval = \"confidence\". predict(model, newdata = newpoints, interval = &quot;confidence&quot;, level = 0.95) 3.5.2 Prediction Intervals Easiest to use predict(). To get a prediciton interval around a new point, use predict() with interval = \"prediction\". predict(model, newdata = newpoints, interval = &quot;prediction&quot;, level = 0.95) 3.5.3 Plotting Confidence and Prediction Intervals Base R To plot a CI or PI line, use seq(a, b, by = ...) and predict() with newdata = data.frame(X = seq(a, b, by = ...)) and then plot the output using abline(). Using interval = \"confidence\" or interval = \"prediction\" depending on if you want a CI or a PI. ggplot Confidence Interval Let se = TRUE in geom_smooth. ggplot Prediction Interval: temp_var &lt;- predict(model, interval=&quot;prediction&quot;) new_df &lt;- cbind(data1, temp_var) ggplot(new_df, aes(x=X, y=Y))+ geom_point() + geom_line(aes(y=lwr), color = &quot;red&quot;, linetype = &quot;dashed&quot;)+ geom_line(aes(y=upr), color = &quot;red&quot;, linetype = &quot;dashed&quot;)+ geom_smooth(method=lm, se=TRUE) 3.6 Misc Pearson Correlation Coefficient (\\(r\\)) cor(A, B) measures linear relationship between A and B and \\(r = \\sqrt{R^2}\\) "],["multiple-linear-regression.html", "Chapter 4 Multiple Linear Regression 4.1 Exploring Data With Multiple Predictor Variables 4.2 Basics of Building an MLR Model (Including Transformations) 4.3 Multicollinearity Detection and VIF 4.4 Model Selection", " Chapter 4 Multiple Linear Regression 4.1 Exploring Data With Multiple Predictor Variables Pairs Plot (Scatterplot Matrix) To see how any pairs of variable in a data are related use pairs(data). For an example with carSales data from Section 3.6, #can make it symmetric on diagonal by leaving as `pairs(carSales)` pairs(carSales, lower.panel = NULL) The upper panel here can be customised by making a new function Example from Section 3.6 # Customize upper panel upper.panel &lt;- function(x, y){ points(x,y, pch=19, col=4) r &lt;- round(cor(x, y), digits=3) txt &lt;- paste0(&quot;r = &quot;, r) usr &lt;- par(&quot;usr&quot;); on.exit(par(usr)) par(usr = c(0, 1, 0, 1)) text(0.5, 0.9, txt) } pairs(carSales, lower.panel = NULL, upper.panel = upper.panel) Example from Practical 2 panel.cor &lt;- function(x, y){ usr &lt;- par(&quot;usr&quot;); on.exit(par(usr)) par(usr = c(0, 1, 0, 1)) r &lt;- round(cor(x, y), digits=2) txt &lt;- paste0(&quot; &quot;, r) text(0.5, 0.5, txt, cex = 0.8) } # Customize upper panel upper.panel&lt;-function(x, y){ points(x,y, pch = 19) } # Create the plots pairs(fat1, lower.panel = panel.cor, upper.panel = upper.panel) Using corrplot Package library(corrplot) Examples from Practical 2 corrplot(cor(fat1), method = &quot;number&quot;, type = &quot;upper&quot;, diag = FALSE) corrplot.mixed(cor(fat1), upper = &quot;ellipse&quot;, lower = &quot;number&quot;,number.cex = .7) Plots To Show Isses Due To Multicollinearity and \\(p\\) close to \\(n\\) To see how to plot how ridge regression lowers variance when we have the problem of multicollinearity and \\(p\\) close to \\(n\\) see notes section 6.2 in notes. 4.2 Basics of Building an MLR Model (Including Transformations) Build a Model With Specific Predictors model &lt;- lm(Y ~ X1 + X2 + ... + Xn, data=data) Build a Model With All Predictors model &lt;- lm(Y ~ ., data=data) Build a Model With Specific Predictors Removed model &lt;- lm(Y ~ . - X1 - X2, data=data) Example Including Transformations model &lt;- lm(Y ~ X1 + X2*X3 + log(X4) + I(X5^2), data=data) Note that X2*X3 includes the predictors X2, X3 and their product X2 x X3. Inference is much the same as with SLR. 4.3 Multicollinearity Detection and VIF To calculate VIF value for each predictor in a model simply use vif() function from car library. library(car) vif(reg) ## Age Miles ## 3.907129 3.907129 4.4 Model Selection The following subsection follows closely to Practical Demonstration 5.4. Most examples will be from there. Best Subset and Forward/Backward Stepwise Selection (Using Selection Criteria) Best Subset Selection is done using regsubsets() function from leaps library. library(leaps) #function for best subset selection best = regsubsets(Salary ~ ., Hitters, nvmax=19) results = summary(best) names(results) #gives the names of the predictors chosen by best subset selection ## [1] &quot;which&quot; &quot;rsq&quot; &quot;rss&quot; &quot;adjr2&quot; &quot;cp&quot; &quot;bic&quot; &quot;outmat&quot; &quot;obj&quot; nvmax=19 specifies that you want all 19 models and not just up to the best one. Data you can extract from output of regsubsets() RSS = results$rss r2 = results$rsq Cp = results$cp BIC = results$bic Adj_r2 = results$adjr2 cbind(RSS, r2, Cp, BIC, Adj_r2) ## RSS r2 Cp BIC Adj_r2 ## [1,] 36179679 0.3214501 104.281319 -90.84637 0.3188503 ## [2,] 30646560 0.4252237 50.723090 -128.92622 0.4208024 ## [3,] 29249297 0.4514294 38.693127 -135.62693 0.4450753 ## [4,] 27970852 0.4754067 27.856220 -141.80892 0.4672734 ## [5,] 27149899 0.4908036 21.613011 -144.07143 0.4808971 ## [6,] 26194904 0.5087146 14.023870 -147.91690 0.4972001 ## [7,] 25906548 0.5141227 13.128474 -145.25594 0.5007849 ## [8,] 25136930 0.5285569 7.400719 -147.61525 0.5137083 ## [9,] 24814051 0.5346124 6.158685 -145.44316 0.5180572 ## [10,] 24500402 0.5404950 5.009317 -143.21651 0.5222606 ## [11,] 24387345 0.5426153 5.874113 -138.86077 0.5225706 ## [12,] 24333232 0.5436302 7.330766 -133.87283 0.5217245 ## [13,] 24289148 0.5444570 8.888112 -128.77759 0.5206736 ## [14,] 24248660 0.5452164 10.481576 -123.64420 0.5195431 ## [15,] 24235177 0.5454692 12.346193 -118.21832 0.5178661 ## [16,] 24219377 0.5457656 14.187546 -112.81768 0.5162219 ## [17,] 24209447 0.5459518 16.087831 -107.35339 0.5144464 ## [18,] 24201837 0.5460945 18.011425 -101.86391 0.5126097 ## [19,] 24200700 0.5461159 20.000000 -96.30412 0.5106270 Plotting To Understand Best Subset Selection #Plots RSS and R-square as you add more predictors par(mfrow = c(1, 2)) plot(RSS, xlab = &quot;Number of Predictors&quot;, ylab = &quot;RSS&quot;, type = &quot;l&quot;, lwd = 2) plot(r2, xlab = &quot;Number of Predictors&quot;, ylab = &quot;R-square&quot;, type = &quot;l&quot;, lwd = 2) Number of predictors in optimal model under different selection criteria which.min(Cp) ## [1] 10 which.min(BIC) ## [1] 6 which.max(Adj_r2) ## [1] 11 Example Plot For How Selection Criteria Change par(mfrow = c(1, 3)) plot(Cp, xlab = &quot;Number of Predictors&quot;, ylab = &quot;Cp&quot;, type = &#39;l&#39;, lwd = 2) points(10, Cp[10], col = &quot;red&quot;, cex = 2, pch = 8, lwd = 2) plot(BIC, xlab = &quot;Number of Predictors&quot;, ylab = &quot;BIC&quot;, type = &#39;l&#39;, lwd = 2) points(6, BIC[6], col = &quot;red&quot;, cex = 2, pch = 8, lwd = 2) plot(Adj_r2, xlab = &quot;Number of Predictors&quot;, ylab = &quot;Adjusted RSq&quot;, type = &quot;l&quot;, lwd = 2) points(11, Adj_r2[11], col = &quot;red&quot;, cex = 2, pch = 8, lwd = 2) Built-in Plot for Criteria Top row is the best model under the criteria and worst is at bottom plot(best, scale = &quot;Cp&quot;) The options for scale are \"bic\", \"Cp\", \"adjr2\" and \"r2\". Extracting Coefficients for Best Model (at a specific number of predictors) coef(best, 10) #Cp ## (Intercept) AtBat Hits Walks CAtBat CRuns ## 162.5354420 -2.1686501 6.9180175 5.7732246 -0.1300798 1.4082490 ## CRBI CWalks DivisionW PutOuts Assists ## 0.7743122 -0.8308264 -112.3800575 0.2973726 0.2831680 The number here which is k=10 corresponds to model M_k. We found this from which.min(Cp). Forward and Backward Stepwise Selection To use forward or backward stepwise selection instead change the method parameter is regsubsets() function. fwd = regsubsets(Salary~., data = Hitters, nvmax = 19, method = &quot;forward&quot;) Best Subset Selection (Using Validation) ‘regsubsets()’ does not have a built-in function to do best subsey selection using validation so we use a custom function (copy and paste). predict.regsubsets = function(object, newdata, id, ...){ form = as.formula(object$call[[2]]) mat = model.matrix(form, newdata) coefi = coef(object, id = id) xvars = names(coefi) mat[, xvars]%*%coefi } object should be the result of a call to regsubsets newdata should be data frame with data from validation set in id specifies we want to use the model from object with id predictors Splitting Data into Training and Validation As a rough guide, split the data approximately 2:1 for Training:Validation. dim(Hitters) training.obs = sample(1:263, 175) Hitters.train = Hitters[training.obs, ] Hitters.test = Hitters[-training.obs, ] Best Subset Validation The regsubsets call: best.val = regsubsets(Salary ~ ., data = Hitters.train, nvmax = 19) Iterate through every number of predictors and calculate the MSE between prediction and observed (from validation set). val.error &lt;- c() for(i in 1:19){ pred = predict.regsubsets(best.val, Hitters.test, i) val.error[i] = mean((Hitters.test$Salary - pred)^2) #MSE } val.error #stores 19 models validation MSE ## [1] 151582.1 161038.3 157865.6 148420.1 148059.5 139724.6 140910.7 136201.2 ## [9] 137252.1 138768.9 138516.7 139014.0 138026.2 140615.3 141701.8 142136.5 ## [17] 142970.9 142813.1 142998.0 Index with smallest MSE is best number of predictors which.min(val.error) ## [1] 8 Inference After choosing optimal model with best subset selection using a validation set, before inference, you must train the model in the entire data set. coef(best.val, 10) # Check which variables to use in the lm. ## (Intercept) AtBat Hits Walks CHmRun CRuns ## 13.9966063 -2.2622740 8.6558865 6.4093820 1.3957008 0.9680297 ## CWalks LeagueN DivisionW PutOuts NewLeagueN ## -0.9134563 152.9904703 -130.3145660 0.2320350 -95.9102264 ls10 = lm(Salary ~ AtBat + Hits + Runs + Walks + CAtBat + CRuns + CRBI + CWalks + Division + PutOuts, data = Hitters) Usual inference can follow. Note: using a validation set means the choice of the best model is random, depending on how you selected which observations were used for training and which were used for validation. Best Subset Selection (Using Cross-Validation) Doing cross-validation is generally preferable to regular validation as it utilises entire data set for training and testing. Continuing with Hitters data from Example 5.4, best = regsubsets(Salary ~ ., data = Hitters, nvmax = 19) This method requires manually specifying the models you include in the cross-validation. So, for this example, we use cross-validation to compare the three models suggested by \\(C_p\\), BIC and adjusted \\(R^2\\). (Recall: could use coef(best,10) to find coefficients in \\(C_p\\) model since \\(C_p\\) found 10 predictors was best). Train Models on Entire Data Set ls10 = lm(Salary ~ AtBat + Hits + Walks + CAtBat + CRuns + CRBI + CWalks + Division + PutOuts + Assists, data = Hitters) ls6 = lm(Salary ~ AtBat + Hits + Walks + CRBI + Division + PutOuts, data = Hitters) ls11 = lm(Salary ~ AtBat + Hits + Walks + CAtBat + CRuns + CRBI + CWalks + + League + Division + PutOuts + Assists, data = Hitters) Creating The Folds Using \\(k = 10\\) folds, k = 10 folds = cut(1:263, breaks=10, labels=FALSE) table(folds) ## folds ## 1 2 3 4 5 6 7 8 9 10 ## 27 26 26 26 27 26 26 26 26 27 However, currently the folds are not random, the first fold is just the first 27 entries. We can randomly shuffle the folds using sample. set.seed(2) folds = sample(folds) folds ## [1] 8 10 8 7 3 5 3 6 9 2 3 4 5 9 7 2 10 5 7 6 2 10 8 6 4 ## [26] 1 5 8 7 5 10 4 5 10 8 8 1 5 9 2 2 10 8 10 1 1 7 9 2 6 ## [51] 5 8 3 4 8 2 1 6 8 10 5 7 2 3 7 7 1 1 7 3 9 8 4 3 6 ## [76] 7 5 5 10 2 5 6 2 1 4 5 9 1 3 3 8 3 10 8 6 1 2 6 1 4 ## [101] 9 6 4 10 9 1 9 7 9 8 4 8 6 1 10 2 10 10 7 9 3 9 4 4 5 ## [126] 6 10 3 9 2 8 8 6 2 2 6 1 9 1 10 7 4 2 4 8 5 8 1 6 3 ## [151] 1 10 10 1 3 3 3 7 3 4 2 9 4 6 8 9 7 2 1 7 4 3 5 7 8 ## [176] 4 9 7 9 5 2 1 1 6 4 4 5 10 6 5 1 10 9 7 3 1 5 2 7 7 ## [201] 4 6 7 10 3 6 5 10 4 9 9 5 7 7 2 6 9 5 5 3 2 3 5 5 9 ## [226] 3 6 1 1 9 8 6 8 9 10 4 4 10 10 5 2 3 3 6 3 4 10 9 6 2 ## [251] 1 8 6 7 8 4 2 8 1 10 2 7 4 Calculating The CV Errors We first create a matrix to store the CV errors, cv.errors = matrix(NA, nrow = k, ncol = 3, dimnames = list(NULL, c(&quot;ls10&quot;, &quot;ls6&quot;, &quot;ls11&quot;))) cv.errors ## ls10 ls6 ls11 ## [1,] NA NA NA ## [2,] NA NA NA ## [3,] NA NA NA ## [4,] NA NA NA ## [5,] NA NA NA ## [6,] NA NA NA ## [7,] NA NA NA ## [8,] NA NA NA ## [9,] NA NA NA ## [10,] NA NA NA Then, iterating through each fold, we obtain the CV errors, for(i in 1:k){ #Trains models on all folds but i ls10 = lm(Salary ~ AtBat + Hits + Walks + CAtBat + CRuns + CRBI + CWalks + Division + PutOuts + Assists, data = Hitters[folds!=i, ] ) ls6 = lm(Salary ~ AtBat + Hits + Walks + CRBI + Division + PutOuts, data = Hitters[folds!=i, ]) ls11 = lm(Salary ~ AtBat + Hits + Walks + CAtBat + CRuns + CRBI + CWalks + + League + Division + PutOuts + Assists, data = Hitters[folds!=i, ]) #Predicts values in fold i pred10 &lt;- predict( ls10, newdata = Hitters[folds==i, ] ) pred6 &lt;- predict( ls6, newdata = Hitters[folds==i, ] ) pred11 &lt;- predict( ls11, newdata = Hitters[folds==i, ] ) #Calculates CV errors cv.errors[i,] = c( mean( (Hitters$Salary[folds==i]-pred10)^2), #calculate MSE for cv mean( (Hitters$Salary[folds==i]-pred6)^2), mean( (Hitters$Salary[folds==i]-pred11)^2) ) } cv.errors ## ls10 ls6 ls11 ## [1,] 132495.25 127446.51 130581.86 ## [2,] 111992.83 110608.25 109426.81 ## [3,] 87822.97 98247.02 87132.03 ## [4,] 113161.24 98812.43 112384.13 ## [5,] 225490.54 231835.72 227007.73 ## [6,] 83622.26 46935.72 85460.28 ## [7,] 90430.09 95275.05 91963.45 ## [8,] 100105.45 106254.47 98519.80 ## [9,] 97701.55 97551.93 104063.92 ## [10,] 48005.26 63602.55 46157.69 Calculating the means gives a performance metric for each model. cv.mean.errors &lt;- colMeans(cv.errors) cv.mean.errors ## ls10 ls6 ls11 ## 109082.7 107657.0 109269.8 "],["shrinkage-methods.html", "Chapter 5 Shrinkage Methods 5.1 Ridge and Lasso Regression 5.2 Principal Component Analysis 5.3 Comparing Predictive Performances", " Chapter 5 Shrinkage Methods 5.1 Ridge and Lasso Regression Performing Ridge Regression To perform ridge regression, we use the package glmnet. library(glmnet) Important Note: glmnet() uses different syntax than regsubsets(). Continuing example with Hitters data, y = Hitters$Salary # Here we exclude the first column because it corresponds to the intercept. x = model.matrix(Salary ~ ., Hitters)[,-1] Note that model.matrix(Salary ~ ., Hitters)[,-1] is exactly the same as Hitters[,-19] (Salary has index 19), it just is all the data for the prediction variables. Performing ridge regression just involves using the glmnet() function specifying alpha = 0 (for ridge). ridge = glmnet(x, y, alpha = 0) To use a different number of \\(\\lambda\\), change parameter nlambda which is 100 by default. Extracting Information names(ridge) ## [1] &quot;a0&quot; &quot;beta&quot; &quot;df&quot; &quot;dim&quot; &quot;lambda&quot; &quot;dev.ratio&quot; ## [7] &quot;nulldev&quot; &quot;npasses&quot; &quot;jerr&quot; &quot;offset&quot; &quot;call&quot; &quot;nobs&quot; ridge$beta contains the values of the coefficients under each \\(\\lambda\\) (use ridge$beta[,1:3] to see first 3 for example). coef(ridge) contains exact same information as ridge$beta but also includes intercept estimates. ridge$lambda contains the grid of all \\(\\lambda\\) values that were used. 5.1.1 Performing Lasso regression Very similar to ridge but with alpha = 1 in glmnet(), which is the default so doesn’t need to be specified. y = Hitters$Salary x = model.matrix(Salary~., Hitters)[,-1] lasso = glmnet(x, y) Plotting Regularisation Paths xvar = 'lambda' specifies to plot coefficient values against Log \\(\\lambda\\), otherwise the default xvar is the \\(L_1\\) Norm. Below are two examples, it is the exact same method for ridge and lasso Ridge Coefficients Against Log \\(\\lambda\\) plot(ridge, xvar = &#39;lambda&#39;) Lasso Coefficients Against \\(L_1\\) Norm plot(lasso) Cross-Validation To find best \\(\\lambda\\) with cross-validation, use cv.glmnet() instead. ridge.cv = cv.glmnet(x, y, alpha=0) names(ridge.cv) ## [1] &quot;lambda&quot; &quot;cvm&quot; &quot;cvsd&quot; &quot;cvup&quot; &quot;cvlo&quot; ## [6] &quot;nzero&quot; &quot;call&quot; &quot;name&quot; &quot;glmnet.fit&quot; &quot;lambda.min&quot; ## [11] &quot;lambda.1se&quot; &quot;index&quot; ridge.cv$lambda.min gives the optimal \\(\\lambda\\). ridge.cv$lambda.1se gives the maximum \\(\\lambda\\) 1 standard-error away from optimal lambda. Plotting In the plot below, the left dotted line highlights value of lambda.min and the right dotted line hightlights value of lambda.1se. plot(ridge.cv) abline( h = ridge.cv$cvup[ridge.cv$index[1]], lty = 4 ) To add these to the plots of coefficients against Log \\(\\lambda\\), plot(ridge, xvar = &#39;lambda&#39;) abline(v = log(ridge.cv$lambda.min), lty = 3) # careful to use the log here and below abline(v = log(ridge.cv$lambda.1se), lty = 3) 5.1.2 Comparing Predictive Performance For Different \\(\\lambda\\)s See another example of this method applied to BSS, Ridge, Lasso and PCR at end of this chapter. repetitions = 50 mse.1 = c() mse.2 = c() set.seed(1) for(i in 1:repetitions){ # Step (i) random data splitting training.obs = sample(1:263, 175) y.train = Hitters$Salary[training.obs] x.train = model.matrix(Salary~., Hitters[training.obs, ])[,-1] y.test = Hitters$Salary[-training.obs] x.test = model.matrix(Salary~., Hitters[-training.obs, ])[,-1] # Step (ii) training phase lasso.train = cv.glmnet(x.train, y.train) # Step (iii) generating predictions predict.1 = predict(lasso.train, x.test, s = &#39;lambda.min&#39;) predict.2 = predict(lasso.train, x.test, s = &#39;lambda.1se&#39;) # Step (iv) evaluating predictive performance mse.1[i] = mean((y.test-predict.1)^2) mse.2[i] = mean((y.test-predict.2)^2) } boxplot(mse.1, mse.2, names = c(&#39;min-CV lasso&#39;,&#39;1-se lasso&#39;), ylab = &#39;Test MSE&#39;, col = 7) 5.2 Principal Component Analysis This section contains many examples from practical 3. We use seatpos (38x9 dimension and no missing values) data from faraway package. library(faraway) To perform principal component analysis without cross-validation, use prcomp() function. Seperate out response and prediction variables into x and y y &lt;- seatpos$hipcenter x &lt;- model.matrix(hipcenter ~ ., seatpos)[,-1] Note that model.matrix(hipcenter ~ ., seatpos)[,-1] is equivalent to seatpos[,-9]. seatpos.pr &lt;- prcomp(x, scale=TRUE) Scaling - scale = TRUE scales each variable by it’s standard error which is needed for meaningful inference. However, this can also be done manually, - To manually scale before running prcomp, divide by column standard errors, s &lt;- apply(x, 2, sd) # calculates the column SDs x.s &lt;- sweep(x, 2, s, &quot;/&quot;) # divides all columns by their SDs Extracting Data Variance: seatpos.pr$sdev gives the standard deviation of each component (seatpos.pr$sdev^2 for variance) Eigenvectors: seatpos.pr$rotation gives the eigenvectors of each component These two are equivalent to calling ...$values and ...$vectors on eigen(var(seatpos.s)) where seatpos.s is seatpos manually scaled (alternatively, don’t scale when calling prcom()). Scree Plots and Proportion of Variance Explained To manually calculate the proportion of variance each PC explains use, seatpos.pr$sdev^2 / sum(seatpos.pr$sdev^2 ) ## [1] 0.7091482088 0.1545982512 0.0579679303 0.0301197935 0.0242774455 ## [6] 0.0173967750 0.0062930659 0.0001985298 Or can directly read off the summary (including cumulative proportions), summary(seatpos.pr) ## Importance of components: ## PC1 PC2 PC3 PC4 PC5 PC6 PC7 ## Standard deviation 2.3818 1.1121 0.68099 0.49088 0.44070 0.3731 0.22438 ## Proportion of Variance 0.7091 0.1546 0.05797 0.03012 0.02428 0.0174 0.00629 ## Cumulative Proportion 0.7091 0.8638 0.92171 0.95183 0.97611 0.9935 0.99980 ## PC8 ## Standard deviation 0.03985 ## Proportion of Variance 0.00020 ## Cumulative Proportion 1.00000 This is the information a Scree Plot shows which can be plotted straight from the call to prcomp(), plot(seatpos.pr) Data Compression (Projection) Scale data beforehand using manual scaling (see above), x.s &lt;- sweep(x, 2, apply(x, 2, sd) , &quot;/&quot;) #scaled data seatpos.pr &lt;- prcomp(x.s) Compress data to \\(k\\) PCs, calculate their means and then reconstruct the data (with error), T &lt;- t(seatpos.pr$x[,c(1,2,3,4)]) # Compressed using 4 PCs ms &lt;- colMeans(x.s) # calculates means of scaled data set R &lt;- t(ms + seatpos.pr$rot[,c(1,2,3,3)]%*% T) # reconstruction plot(rbind(x.s[,1:2], R[,1:2]), col=c(rep(4,38),rep(2,38))) Original data is blue, reconstructions are red Note: The above only plots two variables, we can plot all pairs using pairs(rbind(x.s, R), col=c(rep(4,38),rep(2,38))) Principal Component Regression To perform principal component regression we use pls package. library(pls) PCR is done with the pcr(function). We continue with Hitters data, pcr.fit = pcr( Salary ~ ., data = Hitters, scale = TRUE, validation = &quot;CV&quot; ) summary(pcr.fit) ## Data: X dimension: 263 19 ## Y dimension: 263 1 ## Fit method: svdpc ## Number of components considered: 19 ## ## VALIDATION: RMSEP ## Cross-validated using 10 random segments. ## (Intercept) 1 comps 2 comps 3 comps 4 comps 5 comps 6 comps ## CV 452 352.5 351.6 352.3 350.7 346.1 345.5 ## adjCV 452 352.1 351.2 351.8 350.1 345.5 344.6 ## 7 comps 8 comps 9 comps 10 comps 11 comps 12 comps 13 comps ## CV 345.4 348.5 350.4 353.2 354.5 357.5 360.3 ## adjCV 344.5 347.5 349.3 351.8 353.0 355.8 358.5 ## 14 comps 15 comps 16 comps 17 comps 18 comps 19 comps ## CV 352.4 354.3 345.6 346.7 346.6 349.4 ## adjCV 350.2 352.3 343.6 344.5 344.3 346.9 ## ## TRAINING: % variance explained ## 1 comps 2 comps 3 comps 4 comps 5 comps 6 comps 7 comps 8 comps ## X 38.31 60.16 70.84 79.03 84.29 88.63 92.26 94.96 ## Salary 40.63 41.58 42.17 43.22 44.90 46.48 46.69 46.75 ## 9 comps 10 comps 11 comps 12 comps 13 comps 14 comps 15 comps ## X 96.28 97.26 97.98 98.65 99.15 99.47 99.75 ## Salary 46.86 47.76 47.82 47.85 48.10 50.40 50.55 ## 16 comps 17 comps 18 comps 19 comps ## X 99.89 99.97 99.99 100.00 ## Salary 53.01 53.85 54.61 54.61 Optimal Number of PCs This is not built in so can be read off from summary above or copy and paste this code, min.pcr = which.min( MSEP( pcr.fit )$val[1,1, ] ) - 1 min.pcr ## 7 comps ## 7 MSE Validation Plot validationplot( pcr.fit, val.type = &#39;MSEP&#39; ) Coefficients of Optimal Number of PCs coef(pcr.fit, ncomp = min.pcr) ## , , 7 comps ## ## Salary ## AtBat 27.005477 ## Hits 28.531195 ## HmRun 4.031036 ## Runs 29.464202 ## RBI 18.974255 ## Walks 47.658639 ## Years 24.125975 ## CAtBat 30.831690 ## CHits 32.111585 ## CHmRun 21.811584 ## CRuns 34.054133 ## CRBI 28.901388 ## CWalks 37.990794 ## LeagueN 9.021954 ## DivisionW -66.069150 ## PutOuts 74.483241 ## Assists -3.654576 ## Errors -6.004836 ## NewLeagueN 11.401041 Similarly, familiar functions like predict can also be used on pcr.fit and has an ncomp parameter. Regularisation Path Plots Full explanation of code is at the end of section 7.6. coef.mat = matrix(NA, 19, 19) for(i in 1:19){ coef.mat[,i] = pcr.fit$coefficients[,,i] } plot(coef.mat[1,], type = &#39;l&#39;, ylab = &#39;Coefficients&#39;, xlab = &#39;Number of components&#39;, ylim = c(min(coef.mat), max(coef.mat))) for(i in 2:19){ lines(coef.mat[i,], col = i) } abline(v = min.pcr, lty = 3) Scree Plots with PCR (manually) PVE &lt;- rep(NA,19) for(i in 1:19){ PVE[i]&lt;- sum(pcr.fit$Xvar[1:i])/pcr.fit$Xtotvar } barplot( PVE, names.arg = 1:19, main = &quot;scree plot&quot;, xlab = &quot;number of PCs&quot;, ylab = &quot;proportion of variance explained&quot; ) 5.3 Comparing Predictive Performances Example for practical 3. Uses leaps, pls and glmnet packages and we’re give we want to split data training:test as 28:10. We also need to include predict.regsubsets from chapter 4. predict.regsubsets = function(object, newdata, id, ...){ form = as.formula(object$call[[2]]) mat = model.matrix(form, newdata) coefi = coef(object, id = id) xvars = names(coefi) mat[, xvars]%*%coefi } We want to calculate the correlation and MSE for the four models BSS (\\(C_p\\)), Ridge (5 folds CV), Lasso (5 fold CV) and PCR over 50 repetitions. cor.bss = c() cor.ridge = c() cor.lasso = c() cor.pcr = c() mse.bss = c() mse.ridge = c() mse.lasso = c() mse.pcr = c() for(i in 1:repetitions){ # Step (i) data splitting training.obs = sample(1:38, 28) y.train = seatpos$hipcenter[training.obs] x.train = model.matrix(hipcenter ~ ., seatpos[training.obs, ])[,-1] y.test = seatpos$hipcenter[-training.obs] x.test = model.matrix(hipcenter ~ ., seatpos[-training.obs, ])[,-1] # Step (ii) training phase bss.train = regsubsets(hipcenter ~ ., seatpos, nvmax = 8) min.cp = which.min(summary(bss.train)$cp) ridge.train = cv.glmnet(x.train, y.train, alpha = 0, nfold=5) lasso.train = cv.glmnet(x.train, y.train, nfold=5) pcr.train = pcr(hipcenter ~ ., data = seatpos[training.obs,], scale = TRUE, validation = &quot;CV&quot; ) min.pcr = which.min(MSEP(pcr.train)$val[1,1, ] ) - 1 # Step (iii) generating predictions predict.bss = predict.regsubsets(bss.train, seatpos[-training.obs, ], min.cp) predict.ridge = predict(ridge.train, x.test, s = &#39;lambda.min&#39;) predict.lasso = predict(lasso.train, x.test, s = &#39;lambda.min&#39;) predict.pcr = predict(pcr.train, seatpos[-training.obs, ], ncomp = min.pcr ) # Step (iv) evaluating predictive performance mse.bss[i] = mean((y.test-predict.bss)^2) mse.ridge[i] = mean((y.test-predict.ridge)^2) mse.lasso[i] = mean((y.test-predict.lasso)^2) mse.pcr[i] = mean((y.test-predict.pcr)^2) cor.bss[i] = cor(y.test, predict.bss) cor.ridge[i] = cor(y.test, predict.ridge) cor.lasso[i] = cor(y.test, predict.lasso) cor.pcr[i] = cor(y.test, predict.pcr) } We can then compare with boxplots boxplot(mse.bss, mse.ridge, mse.lasso, mse.pcr, names = c(&#39;BSS&#39;,&#39;Ridge&#39;,&#39;Lasso&#39;,&#39;PCR&#39;), ylab = &#39;Test MSE&#39;, col = 2:5) boxplot(cor.bss, cor.ridge, cor.lasso, cor.pcr, names = c(&#39;BSS&#39;,&#39;Ridge&#39;,&#39;Lasso&#39;,&#39;PCR&#39;), ylab = &#39;Test Correlation&#39;, col = 2:5) "],["polynomial-regression.html", "Chapter 6 Polynomial Regression 6.1 Polynomial Regression Model Example (Degree 2) 6.2 Polynomial Regression (Higher Degrees) 6.3 Choosing a Degree (ANOVA) 6.4 Polynomial Regression with Multiple Predictors", " Chapter 6 Polynomial Regression Simplest example of a non-linear model. 6.1 Polynomial Regression Model Example (Degree 2) Following practical demonstration 8.3 we build a model for Boston dataset in library(MASS) which has no missing data. To just see an example see Polynomial Regression (Higher Degrees). ## ## Attaching package: &#39;MASS&#39; ## The following object is masked from &#39;package:dplyr&#39;: ## ## select For convenience, we can rename the response and predictor variable y and x respectively, and label axes for plotting. Below is then a basic plot of response against predictor. y = Boston$medv x = Boston$lstat y.lab = &#39;Median Property Value&#39; x.lab = &#39;Lower Status (%)&#39; plot( x, y, cex.lab = 1.1, col=&quot;darkgrey&quot;, xlab = x.lab, ylab = y.lab, main = &quot;&quot;, bty = &#39;l&#39; ) Polynomial Regression Function Polynomial regression can be called by writing out the predictors directly (poly2 = lm( y ~ x + I(x^2) )), or using the poly() command. poly2 = lm(y ~ poly(x, 2, raw = TRUE)) #2 is the degree #summary(poly2) Here, raw = TRUE calculates the polynomial regression as usual. Leaving this out so default is raw = FALSE would lead to an orthogonal basis being chosen to perform the regression on. Plotting Simple Polynomial Regression First, we mus sort the \\(x\\) values for the plot to work. We use each initial \\(x\\) data point as a point we connect between to draw the polynomial. sort.x = sort(x) pred2 = predict(poly2, newdata = list(x = sort.x), se = TRUE) names(pred2) ## [1] &quot;fit&quot; &quot;se.fit&quot; &quot;df&quot; &quot;residual.scale&quot; pred2 contains fit which are the fitted values and se.fit which are the standard errors at each fitted point. se.bands2 = cbind( pred2$fit - 2 * pred2$se.fit, pred2$fit + 2 * pred2$se.fit ) Final Plot plot(x, y, cex.lab = 1.1, col=&quot;darkgrey&quot;, xlab = x.lab, ylab = y.lab, main = &quot;Degree-2 polynomial&quot;, bty = &#39;l&#39;) lines(sort.x, pred2$fit, lwd = 2, col = &quot;red&quot;) matlines(sort.x, se.bands2, lwd = 1.4, col = &quot;red&quot;, lty = 3) 6.2 Polynomial Regression (Higher Degrees) poly3 = lm(y ~ poly(x, 3)) poly4 = lm(y ~ poly(x, 4)) poly5 = lm(y ~ poly(x, 5)) pred3 = predict(poly3, newdata = list(x = sort.x), se = TRUE) pred4 = predict(poly4, newdata = list(x = sort.x), se = TRUE) pred5 = predict(poly5, newdata = list(x = sort.x), se = TRUE) se.bands3 = cbind(pred3$fit + 2*pred3$se.fit, pred3$fit-2*pred3$se.fit) se.bands4 = cbind(pred4$fit + 2*pred4$se.fit, pred4$fit-2*pred4$se.fit) se.bands5 = cbind(pred5$fit + 2*pred5$se.fit, pred5$fit-2*pred5$se.fit) par(mfrow = c(2,2)) # Degree-2 plot(x, y, cex.lab = 1.1, col=&quot;darkgrey&quot;, xlab = x.lab, ylab = y.lab, main = &quot;Degree-2 polynomial&quot;, bty = &#39;l&#39;) lines(sort.x, pred2$fit, lwd = 2, col = &quot;red&quot;) matlines(sort.x, se.bands2, lwd = 2, col = &quot;red&quot;, lty = 3) # Degree-3 plot(x, y, cex.lab = 1.1, col=&quot;darkgrey&quot;, xlab = x.lab, ylab = y.lab, main = &quot;Degree-3 polynomial&quot;, bty = &#39;l&#39;) lines(sort.x, pred3$fit, lwd = 2, col = &quot;darkviolet&quot;) matlines(sort.x, se.bands3, lwd = 2, col = &quot;darkviolet&quot;, lty = 3) # Degree-4 plot(x, y, cex.lab = 1.1, col=&quot;darkgrey&quot;, xlab = x.lab, ylab = y.lab, main = &quot;Degree-4 polynomial&quot;, bty = &#39;l&#39;) lines(sort.x, pred4$fit, lwd = 2, col = &quot;blue&quot;) matlines(sort.x, se.bands4, lwd = 2, col = &quot;blue&quot;, lty = 3) # Degree-5 plot(x, y, cex.lab = 1.1, col=&quot;darkgrey&quot;, xlab = x.lab, ylab = y.lab, main = &quot;Degree-5 polynomial&quot;, bty = &#39;l&#39;) lines(sort.x, pred5$fit, lwd = 2, col = &quot;black&quot;) matlines(sort.x, se.bands5, lwd = 2, col = &quot;black&quot;, lty = 3) 6.3 Choosing a Degree (ANOVA) This can be done by F-test comparison (analysis of variance - ANOVA) or Cross-Validation. With all the polynomial models trained, we can compare them with the function anova(). anova(poly1, poly2, poly3, poly4, poly5, poly6) ## Analysis of Variance Table ## ## Model 1: y ~ x ## Model 2: y ~ poly(x, 2, raw = TRUE) ## Model 3: y ~ poly(x, 3) ## Model 4: y ~ poly(x, 4) ## Model 5: y ~ poly(x, 5) ## Model 6: y ~ poly(x, 6) ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 504 19472 ## 2 503 15347 1 4125.1 151.8623 &lt; 2.2e-16 *** ## 3 502 14616 1 731.8 26.9390 3.061e-07 *** ## 4 501 13968 1 647.8 23.8477 1.406e-06 *** ## 5 500 13597 1 370.7 13.6453 0.0002452 *** ## 6 499 13555 1 42.4 1.5596 0.2123125 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Looking at the column Pr(&gt;F), reading down the column, if it is less than a significance level (say 0.05) then accept adding a higher degree and if it isn’t stop on whatever degree model you have. So we would end up with a degree 5 polynomial model in this example. 6.4 Polynomial Regression with Multiple Predictors x1=Boston$lstat x2=Boston$rm With multiple predictor variables, we can either manually add their interaction variables (polym1 &lt;- lm(y ~ poly(x1, 2) + poly(x2 , 2) + x1:x2)) or use polym() command, polym2=lm(y ~ polym(x1, x2, degree=2) ) "],["splines.html", "Chapter 7 Splines 7.1 Step Functions 7.2 Regression Splines 7.3 Natural Splines 7.4 Smoothing Splines 7.5 Comparing Splines", " Chapter 7 Splines See final subsection on comparing splines for best way to plot this kind of data. 7.1 Step Functions With Boston data, following on from polynomial regression section. To use a step function model, use cut(x, n) function which automatically assigns break points. Note that n specifies the number of intervals, not the number of break points (cuts). step3 = lm(y ~ cut(x, 3)) Then following generic analysis for plotting: pred3 = predict(step3, newdata = list(x = sort(x)), se = TRUE) se.bands3 = cbind(pred3$fit + 2*pred3$se.fit, pred3$fit-2*pred3$se.fit) plot(x, y, cex.lab = 1.1, col=&quot;darkgrey&quot;, xlab = x.lab, ylab = y.lab, main = &quot;2 cutpoints&quot;, bty = &#39;l&#39;) lines(sort(x), pred3$fit, lwd = 2, col = &quot;darkviolet&quot;) matlines(sort(x), se.bands3, lwd = 1.4, col = &quot;darkviolet&quot;, lty = 3) We plot below as if we did it for 2,3,4 and 5 intervals: 7.2 Regression Splines Using Boston data again with x and y defined as variables for convenience with labels y.lab and x.lab done as well. For regression splines, we use the splines package. library(splines) Cuts for regression splines can be specified or done automatically. One option is to specify them using quartiles: cuts = summary(x)[c(2, 3, 5)]. Since we use data points \\(x\\) values as points to plot the splines, we must also sort them: sort.x = sort(x). Regression Spline Model and SE Bands We use bs() function from splines and can either specify the knots or specify the degrees of freedom: spline1 = lm(y ~ bs(x, degree = 1, knots = cuts)) pred1 = predict(spline1, newdata = list(x = sort.x), se = TRUE) se.bands1 = cbind(pred1$fit + 2 * pred1$se.fit, pred1$fit - 2 * pred1$se.fit) spline1df = lm(y ~ bs(x, degree = 1, df = 5)) pred1df = predict(spline1df, newdata = list(x = sort.x), se = TRUE) se.bands1df = cbind( pred1df$fit + 2 * pred1df$se.fit, pred1df$fit - 2 * pred1df$se.fit ) Plot par(mfrow = c(1, 2)) plot(x, y, cex.lab = 1.1, col=&quot;darkgrey&quot;, xlab = x.lab, ylab = y.lab, main = &quot;Linear Spline (with knots)&quot;, bty = &#39;l&#39;) lines(sort.x, pred1$fit, lwd = 2, col = &quot;red&quot;) matlines(sort.x, se.bands1, lwd = 2, col = &quot;red&quot;, lty = 3) plot(x, y, cex.lab = 1.1, col=&quot;darkgrey&quot;, xlab = x.lab, ylab = y.lab, main = &quot;Linear Spline (with df)&quot;, bty = &#39;l&#39;) lines(sort.x, pred1df$fit, lwd = 2, col = &quot;darkred&quot;) matlines(sort.x, se.bands1df, lwd = 2, col = &quot;red&quot;, lty = 3) For Higher Degrees spline2 = lm(y ~ bs(x, degree = 2, df = 6)) pred2 = predict(spline2, newdata = list(x = sort.x), se = TRUE) se.bands2 = cbind(pred2$fit + 2 * pred2$se.fit, pred2$fit - 2 * pred2$se.fit) spline3 = lm(y ~ bs(x, degree = 3, df = 7)) pred3 = predict(spline3, newdata = list(x = sort.x), se = TRUE) se.bands3 = cbind(pred3$fit + 2 * pred3$se.fit, pred3$fit - 2 * pred3$se.fit) par(mfrow = c(1,3)) plot(x, y, cex.lab = 1.1, col=&quot;darkgrey&quot;, xlab = x.lab, ylab = y.lab, main = &quot;Linear Spline&quot;, bty = &#39;l&#39;) lines(sort.x, pred1$fit, lwd = 2, col = &quot;darkred&quot;) matlines(sort.x, se.bands1, lwd = 2, col = &quot;darkred&quot;, lty = 3) plot(x, y, cex.lab = 1.1, col=&quot;darkgrey&quot;, xlab = x.lab, ylab = y.lab, main = &quot;Quadratic Spline&quot;, bty = &#39;l&#39;) lines(sort.x, pred2$fit, lwd = 2, col = &quot;darkgreen&quot;) matlines(sort.x, se.bands2, lwd = 2, col = &quot;darkgreen&quot;, lty = 3) plot(x, y, cex.lab = 1.1, col=&quot;darkgrey&quot;, xlab = x.lab, ylab = y.lab, main = &quot;Cubic Spline&quot;, bty = &#39;l&#39;) lines(sort.x, pred3$fit, lwd = 2, col = &quot;darkblue&quot;) matlines(sort.x, se.bands3, lwd = 2, col = &quot;darkblue&quot;, lty = 3) 7.3 Natural Splines Adds the extra constraints of continuity in first \\(d-1\\) derivatives. We use the ns() function. Again, we use library(\"splines\") package. spline.ns3 = lm(y ~ ns(x, df = 3)) pred.ns3 = predict(spline.ns3, newdata = list(x = sort.x), se = TRUE) se.bands.ns3 = cbind(pred.ns3$fit + 2 * pred.ns3$se.fit, pred.ns3$fit - 2 * pred.ns3$se.fit) Plotting Assuming we have made similarly spline.ns1, spline.ns2, spline.ns3 and spline.ns4. par(mfrow = c(2, 2)) plot(x, y, cex.lab = 1.1, col=&quot;darkgrey&quot;, xlab = x.lab, ylab = y.lab, main = &quot;Natural Spline (1 df)&quot;, bty = &#39;l&#39;) lines(sort.x, pred.ns1$fit, lwd = 2, col = &quot;darkred&quot;) matlines(sort.x, se.bands.ns1, lwd = 2, col = &quot;darkred&quot;, lty = 3) plot(x, y, cex.lab = 1.1, col=&quot;darkgrey&quot;, xlab = x.lab, ylab = y.lab, main = &quot;Natural Spline (2 df)&quot;, bty = &#39;l&#39;) lines(sort.x, pred.ns2$fit, lwd = 2, col = &quot;darkgreen&quot;) matlines(sort.x, se.bands.ns2, lwd = 2, col = &quot;darkgreen&quot;, lty = 3) plot(x, y, cex.lab = 1.1, col=&quot;darkgrey&quot;, xlab = x.lab, ylab = y.lab, main = &quot;Natural Spline (3 df)&quot;, bty = &#39;l&#39;) lines(sort.x, pred.ns3$fit, lwd = 2, col = &quot;darkblue&quot;) matlines(sort.x, se.bands.ns3, lwd = 2, col = &quot;darkblue&quot;, lty = 3) plot(x, y, cex.lab = 1.1, col=&quot;darkgrey&quot;, xlab = x.lab, ylab = y.lab, main = &quot;Natural Spline (4 df)&quot;, bty = &#39;l&#39;) lines(sort.x, pred.ns4$fit, lwd = 2, col = &quot;brown&quot;) matlines(sort.x, se.bands.ns4, lwd = 2, col = &quot;brown&quot;, lty = 3) Practical 4 Example (Using Knots) ns = lm(y ~ ns(x, knots = cuts)) pred.ns = predict(ns, newdata = list(x = grid), se = TRUE) se.bands.ns = cbind(pred.ns$fit + 2 * pred.ns$se.fit, pred.ns$fit - 2 * pred.ns$se.fit) 7.4 Smoothing Splines For smoothing splines we use smoothing.splines() instead of lm(). Use the df parameter to specify effective degrees of freedom or the cv = TRUE parameter to use (ordinary leave-one-out) cross validation instead (to choose a \\(\\lambda\\)). Can also directly specify lambda or spar (a smoothing parameter). Notes: Specifying \\(\\lambda\\) with effective degrees of freedom (df = ...) still uses cross-validation. smooth1 = smooth.spline(x, y, df = 3) #choose lambda using effective degrees of freedom smooth2 = smooth.spline(x, y, cv = TRUE) #choose lambda via cross validation par(mfrow = c(1,2)) plot(x, y, cex.lab = 1.1, col=&quot;darkgrey&quot;, xlab = x.lab, ylab = y.lab, main = &quot;Smoothing Spline (3 df)&quot;, bty = &#39;l&#39;) lines(smooth1, lwd = 2, col = &quot;brown&quot;) plot(x, y, cex.lab = 1.1, col=&quot;darkgrey&quot;, xlab = x.lab, ylab = y.lab, main = &quot;Smoothing Spline (CV)&quot;, bty = &#39;l&#39;) lines(smooth2, lwd = 2, col = &quot;darkorange&quot;) 7.5 Comparing Splines Below compares different splines on wage data. We load packages library(\"ISLR\") for data and library(\"splines\") to use regression and natural splines. Setup for plotting curves agelims = range(Wage$age) age.grid = seq(from=agelims[1], to=agelims[2]) Taking a sample of data of size \\(n\\) set.seed(1) # Number of data points - change this to investigate small (50), # medium (200) and large (1000) samples. n &lt;- 200 ind = sample(1:3000, n) Wage1 = Wage[ind,] # Label subset of data as Wage1. Cubic Spline fitbs = lm(wage~bs(age, degree = 3, knots = c(30,40,60)), data = Wage1) predbs = predict(fitbs, newdata = list(age = age.grid), se = T) Natural Spline fitns = lm(wage~ns(age, knots = c(30,40,60)), data = Wage1) predns = predict(fitns, newdata = list(age = age.grid), se = T) ## Warning in if (se.fit) list(fit = predictor, se.fit = se, df = df, ## residual.scale = sqrt(res.var)) else predictor: the condition has length &gt; 1 and ## only the first element will be used Smoothing Spline fitss = smooth.spline(Wage1$age, Wage1$wage, cv = TRUE) Generate the Plots par(mfrow=c(1,3)) # Cubic Spline plot(Wage1$age, Wage1$wage, col = &quot;darkgray&quot;, pch = 19, main = &#39;Cubic spline&#39;, bty = &#39;l&#39;, xlab = &#39;age&#39;, ylab = &#39;wage&#39;, cex.lab = 1.4) lines(age.grid, predbs$fit, lwd = 2, col = &#39;darkgreen&#39;) abline(v = c(30,40,60), lty = &#39;dashed&#39;) # Natural Spline plot(Wage1$age, Wage1$wage, col = &quot;darkgray&quot;, pch = 19, main = &#39;Natural cubic spline&#39;, bty = &#39;l&#39;, xlab = &#39;age&#39;, ylab = &#39;wage&#39;, cex.lab = 1.4) lines(age.grid, predns$fit, lwd = 2, col = &#39;darkviolet&#39;) abline(v = c(30,40,60), lty = &#39;dashed&#39;) # Smoothing Spline plot(Wage1$age, Wage1$wage, col = &quot;darkgray&quot;, pch = 19, main = &#39;Smoothing spline&#39;, bty = &#39;l&#39;, xlab = &#39;age&#39;, ylab = &#39;wage&#39;, cex.lab = 1.4) lines(fitss, lwd = 2, col = &#39;brown&#39;) Practical 4 Example Modelling medv ~ indus using natural and smoothing splines. Basic Setup: library(&quot;MASS&quot;) library(splines) data(&quot;Boston&quot;) y = Boston$medv x = Boston$indus y.lab = &quot;Median House Value ($1000s)&quot; x.lab = &quot;Proportion of non-retail business acres per town&quot; cuts &lt;- summary(x)[c(2,3,5)] #For knots grid = seq(min(x), max(x), length.out = 500) #For plotting curves Cubic Spline: bs = lm(y ~ bs(x, knots = cuts)) pred.bs = predict(bs, newdata = list(x = grid), se = TRUE) se.bands.bs = cbind(pred.bs$fit + 2 * pred.bs$se.fit, pred.bs$fit - 2 * pred.bs$se.fit) Natural Cubic Spline (Knots): nsk = lm(y ~ ns(x, knots = cuts)) pred.nsk = predict(nsk, newdata = list(x = grid), se = TRUE) se.bands.nsk = cbind(pred.nsk$fit + 2 * pred.nsk$se.fit, pred.nsk$fit - 2 * pred.nsk$se.fit) Natural Cubic Spline (2 df): ns2 = lm(y ~ ns(x, df = 2)) pred.ns2 = predict(ns2, newdata = list(x = grid), se = TRUE) se.bands.ns2 = cbind(pred.ns2$fit + 2 * pred.ns2$se.fit, pred.ns2$fit - 2 * pred.ns2$se.fit) Natural Cubic Spline (3 df): ns3 = lm(y ~ ns(x, df = 3)) pred.ns3 = predict(ns3, newdata = list(x = grid), se = TRUE) se.bands.ns3 = cbind(pred.ns3$fit + 2 * pred.ns3$se.fit, pred.ns3$fit - 2 * pred.ns3$se.fit) Smoothing Spline (3 effective df): smooth = smooth.spline(x, y, df = 3) Plot: par(mfrow = c(2,3)) plot(x, y, col=&quot;darkgrey&quot;, xlab = x.lab, ylab = y.lab, main = &quot;Cubic Spline&quot;, bty = &#39;l&#39;, pch = 16) lines(grid, pred.bs$fit, lwd = 2, col = 1) matlines(grid, se.bands.bs, lwd = 2, col = 1, lty = 3) plot(x, y, col=&quot;darkgrey&quot;, xlab = x.lab, ylab = y.lab, main = &quot;Natural Cubic Spline&quot;, bty = &#39;l&#39;, pch = 16) lines(grid, pred.nsk$fit, lwd = 2, col = 2) matlines(grid, se.bands.nsk, lwd = 2, col = 2, lty = 3) plot(x, y, col=&quot;darkgrey&quot;, xlab = x.lab, ylab = y.lab, main = &quot;Natural Spline (2 df)&quot;, bty = &#39;l&#39;, pch = 16) lines(grid, pred.ns2$fit, lwd = 2, col = 3) matlines(grid, se.bands.ns2, lwd = 2, col = 3, lty = 3) plot(x, y, col=&quot;darkgrey&quot;, xlab = x.lab, ylab = y.lab, main = &quot;Natural Spline (3 df)&quot;, bty = &#39;l&#39;, pch = 16) lines(grid, pred.ns3$fit, lwd = 2, col = 4) matlines(grid, se.bands.ns3, lwd = 2, col = 4, lty = 3) plot(x, y, col=&quot;darkgrey&quot;, xlab = x.lab, ylab = y.lab, main = &quot;Smoothing Spline (3 Effective df)&quot;, bty = &#39;l&#39;, pch = 16) lines(smooth, lwd = 2, col = 6) "],["gams.html", "Chapter 8 GAMs 8.1 GAM Function 8.2 Plotting 8.3 Prediction", " Chapter 8 GAMs We use the library(gam) package. Below is an example using the Boston data again. library(gam) names(Boston) ## [1] &quot;crim&quot; &quot;zn&quot; &quot;indus&quot; &quot;chas&quot; &quot;nox&quot; &quot;rm&quot; &quot;age&quot; ## [8] &quot;dis&quot; &quot;rad&quot; &quot;tax&quot; &quot;ptratio&quot; &quot;black&quot; &quot;lstat&quot; &quot;medv&quot; In this example we build a GAM using a cubic spline with 3 degrees of freedom for lstat, a smoothing spline with 3 degrees of freedom for indus and a simple linear model for variable chas. But first we make chas a categorical variable. Boston1 = Boston Boston1$chas = factor(Boston1$chas) Practical 4 Example library(faraway) data(&quot;seatpos&quot;) gam = gam(hipcenter ~ ns(Age, df = 5) + s(Thigh, df = 3) + Ht, data = seatpos) par( mfrow = c(2,3) ) plot( gam, se = TRUE, col = &quot;blue&quot; ) plot( seatpos$Age, seatpos$hipcenter, pch = 16, col = 2, ylab = y.lab, xlab = &quot;Age (years)&quot; ) plot( seatpos$Thigh, seatpos$hipcenter, pch = 16, col = 2, ylab = y.lab, xlab = &quot;Thigh length (cm)&quot; ) plot( seatpos$Ht, seatpos$hipcenter, pch = 16, col = 2, ylab = y.lab, xlab = &quot;Ht (bare foot) (cm)&quot; ) 8.1 GAM Function We use the gam() function which has the syntax gam( response ~ predictors + ..., data = data). The way to specify what the model of each predictor is is similar to lm(). bs() is used for a regression spline ns() is used for natural splines s() is used for smoothing splines (this is different from lm and how smoothing splines are done normally). gam = gam( medv ~ bs(lstat, degree = 3, df = 5) + s(indus, df = 5) + chas, data = Boston1 ) 8.2 Plotting We have the option to include standard error error bars or not. par( mfrow = c(1,3) ) plot( gam, se = TRUE, col = &quot;blue&quot; ) 8.3 Prediction Prediction works the same as with lm() for multiple linear regression models. preds &lt;- predict( gam, newdata = data.frame( chas = &quot;0&quot;, indus = 3, lstat = 5 ) ) preds ## 1 ## 32.10065 "],["logistic-regression.html", "Chapter 9 Logistic Regression 9.1 Simple Logistic Regression 9.2 Multiple Logistic Regression 9.3 Inference", " Chapter 9 Logistic Regression For estimating probability of a qualitative variable. See MLLN Notes for exploratory plots and motivation. 9.1 Simple Logistic Regression We use the Default data from library(\"ISLR2\") package. We also load library(scales) package for transparency with colours. ## ## Attaching package: &#39;scales&#39; ## The following object is masked from &#39;package:purrr&#39;: ## ## discard ## The following object is masked from &#39;package:readr&#39;: ## ## col_factor ## ## Attaching package: &#39;ISLR2&#39; ## The following objects are masked _by_ &#39;.GlobalEnv&#39;: ## ## Boston, Hitters ## The following object is masked from &#39;package:MASS&#39;: ## ## Boston ## The following objects are masked from &#39;package:ISLR&#39;: ## ## Auto, Credit We use the generalised linear model function glm() with family = \"binomial\". data(Default) glm.fit = glm( as.numeric(Default$default==&quot;Yes&quot;) ~ balance, data = Default, family = &quot;binomial&quot;) summary(glm.fit) ## ## Call: ## glm(formula = as.numeric(Default$default == &quot;Yes&quot;) ~ balance, ## family = &quot;binomial&quot;, data = Default) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.2697 -0.1465 -0.0589 -0.0221 3.7589 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -1.065e+01 3.612e-01 -29.49 &lt;2e-16 *** ## balance 5.499e-03 2.204e-04 24.95 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 2920.6 on 9999 degrees of freedom ## Residual deviance: 1596.5 on 9998 degrees of freedom ## AIC: 1600.5 ## ## Number of Fisher Scoring iterations: 8 summary(glm.fit$fitted.values) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.0000237 0.0003346 0.0021888 0.0333000 0.0142324 0.9810081 Plot #Raw Data plot(Default$balance, as.numeric(Default$default==&quot;Yes&quot;),col=&quot;red&quot;,xlab=&quot;balance&quot;,ylab=&quot;default&quot;) #Fitted Values points(glm.fit$data$balance,glm.fit$fitted.values, col = &quot;black&quot;, pch = 4) #Model Curve curve(predict(glm.fit,data.frame(balance = x),type=&quot;resp&quot;),col=&quot;blue&quot;,lwd=2,add=TRUE) Predicting \\(\\hat{p}(X)\\) X = 1300 p.hat = exp(sum(glm.fit$coefficients*c(1,X)))/(1+exp(sum(glm.fit$coefficients*c(1,X)))) p.hat ## [1] 0.02923441 9.2 Multiple Logistic Regression Pairs Plot (Factor Variable Coloured) pairs(admit[,2:4], col=admit[,1]+2, pch=16) Making a Variable a Factor admit$rank = factor(admit$rank) Practical 4 Example glm.fit = glm( admit ~ ., data = admit, family = &quot;binomial&quot;) summary(glm.fit) ## ## Call: ## glm(formula = admit ~ ., family = &quot;binomial&quot;, data = admit) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.6268 -0.8662 -0.6388 1.1490 2.0790 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -3.989979 1.139951 -3.500 0.000465 *** ## gre 0.002264 0.001094 2.070 0.038465 * ## gpa 0.804038 0.331819 2.423 0.015388 * ## rank2 -0.675443 0.316490 -2.134 0.032829 * ## rank3 -1.340204 0.345306 -3.881 0.000104 *** ## rank4 -1.551464 0.417832 -3.713 0.000205 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 499.98 on 399 degrees of freedom ## Residual deviance: 458.52 on 394 degrees of freedom ## AIC: 470.52 ## ## Number of Fisher Scoring iterations: 4 9.3 Inference Calculating Probability Use predict() function with type = \"response\" or type = \"resp\". Do not include any data to calculate predictions for training data (used to plot the curve) glm.probs = predict(glm.fit, type = &quot;response&quot;) Single Prediction/Multiple New Prediction #Data For Prediction testdata = data.frame(gre = 380, gpa = 3.61, rank = 3) testdata$rank = factor(testdata$rank) #Single Prediction predict(glm.fit, testdata, type=&quot;resp&quot;) ## 1 ## 0.1726265 Predictions Round probability to get a prediction. #Prediction glm.pred = round(glm.probs) #Confusion Table table(glm.pred, admit$admit) ## ## glm.pred 0 1 ## 0 254 97 ## 1 19 30 #Ratio of correct predictions on test data mean(glm.pred == admit$admit) ## [1] 0.71 "],["tree-models.html", "Chapter 10 Tree Models 10.1 Building Tree Models 10.2 Bagging and Random Forests 10.3 Boosting", " Chapter 10 Tree Models 10.1 Building Tree Models library(&quot;tree&quot;) Use tree() function from library(\"tree\"). tree_fit = tree(y ~ x, data_surrogate) #summary(tree_fit) #tree_fit Plotting Trees If pretty = 0 then the level names of a factor split attributes are used unchanged. plot(tree_fit) text(tree_fit, pretty=0) Predictions Example from practical demonstration with synthetic surrogate data. set.seed(347) data_surrogate_test &lt;- data.frame(x = c(runif(200, 0, 0.4), runif(400, 0.4, 0.65), runif(300, 0.65, 1)), y = c(rnorm(200, 1.5), rnorm(400, 0), rnorm(300, 2))) Use predict() function. The tree model gives the mean in that terminal node group as a prediction. tree_pred = predict(tree_fit, data_surrogate_test) Plotting Predictions plot(x = data_surrogate_test$x[1:200], y = data_surrogate_test$y[1:200], col = &#39;blue&#39;, xlab = &quot;X&quot;, ylab = &quot;Y&quot;, pch = 19, xlim = c(0,1), ylim = c(-2,4)) points(x = data_surrogate_test$x[201:600], y = data_surrogate_test$y[201:600], col = &#39;red&#39;, pch = 19) points(x = data_surrogate_test$x[601:900], y = data_surrogate_test$y[601:900], col = &#39;green&#39;, pch = 19) points(x=data_surrogate_test$x,y=tree_pred,col=&#39;black&#39;,pch=15) MSE pred_mse=mean((data_surrogate_test$y-tree_pred)^2) pred_mse ## [1] 1.005645 Pruning Trees Prune tree with prune.tree() command, but can first plot the deviance for different size trees using cv.tree() with parameter FUN = prune.tree. tree_cv_prune = cv.tree(tree_fit_s, FUN = prune.tree) #tree_cv_prune plot(tree_cv_prune) Fitting Pruned Model tree_fit_prune = prune.tree(tree_fit_s, best = 3) tree_fit_prune ## node), split, n, deviance, yval ## * denotes terminal node ## ## 1) root 90 131.60 1.0660 ## 2) x &lt; 0.653469 60 90.67 0.7376 ## 4) x &lt; 0.427732 26 29.65 1.4510 * ## 5) x &gt; 0.427732 34 37.64 0.1917 * ## 3) x &gt; 0.653469 30 21.48 1.7240 * You may also prune the tree by specifying the cost-complexity parameter k, for example tree_fit_prune=prune.tree(tree_fit_s, k=5). Pruned Tree: plot(tree_fit_prune) text(tree_fit_prune,pretty=0) Pruning has decreased the MSE: tree_pred_prune=predict(tree_fit_prune,data_surrogate_test) pred_mse_prune = mean((data_surrogate_test$y-tree_pred_prune)^2) pred_mse - pred_mse_prune ## [1] 0.1765653 The following function shows this is a general result and holds on average: prune_improvement &lt;- function(k) { data_surrogate_s &lt;- data.frame(x = c(runif(20, 0, 0.4), runif(40, 0.4, 0.65), runif(30, 0.65, 1)), y = c(rnorm(20, 1.5), rnorm(40, 0), rnorm(30, 2))) tree_fit_s = tree(y~x,data_surrogate_s) tree_pred_s = predict(tree_fit_s,data_surrogate_test) pred_mse_s = mean((data_surrogate_test$y-tree_pred_s)^2) tree_fit_prune = prune.tree(tree_fit_s,k=5) tree_pred_prune = predict(tree_fit_prune,data_surrogate_test) pred_mse_prune = mean((data_surrogate_test$y-tree_pred_prune)^2) dif_t = pred_mse_s-pred_mse_prune return(dif_t) } mean(sapply(1:1024, FUN=function(i){prune_improvement(5)})) ## [1] 0.09926117 10.2 Bagging and Random Forests Follows an example which uses library(ISLR) and library(MASS) for bagging, random forests and boosted trees. We have data_train and data_test from the Boston dataset. Tree Model and Pruned Tree Model Trained: tree.boston = tree(medv ~ ., data_train) prune.boston = prune.tree(tree.boston, best = 6) Bagging Model For bagging, we use the library(randomForest) package but with mrty = 13 (\\(m = p\\) so using all predictors). Bagging Model (10 Trees): set.seed (472) bag.boston = randomForest(medv ~ ., data = data_train, mtry=13, ntree=10, importance = TRUE) Prediction Accuracy: pred.bag = predict(bag.boston, newdata = data_test) plot(pred.bag, data_test$medv, bty = &#39;l&#39;) abline(0,1) MSE: mean((pred.bag - data_test$medv)^2) ## [1] 23.98799 Same for 100 and 1000 trees: #100 trees bag.boston = randomForest(medv ~ ., data = data_train, mtry = 13, ntree = 100, importance = TRUE) pred.bag = predict(bag.boston, newdata = data_test) mean((pred.bag - data_test$medv)^2) #MSE ## [1] 24.04128 #1000 trees bag.boston = randomForest(medv ~ ., data = data_train, mtry = 13, ntree = 1000, importance = TRUE) pred.bag = predict(bag.boston, newdata = data_test) mean((pred.bag - data_test$medv)^2) #MSE ## [1] 23.46335 Random Forests Use mtry as something else for random forests. By default this will be \\(p/3\\) but can also try \\(\\sqrt{p}\\). Very similar method to bagging. rf.boston = randomForest(medv ~ ., data = data_train, mtry = 4, ntree = 100, importance = TRUE) #uses 4 variables pred.rf = predict(rf.boston, newdata = data_test) mean((pred.rf -data_test$medv)^2) #MSE ## [1] 19.23722 Importance Plot importance(rf.boston) ## %IncMSE IncNodePurity ## crim 5.141187 1206.45846 ## zn 1.862903 216.91995 ## indus 3.527213 657.55620 ## chas 1.109467 43.61585 ## nox 5.448503 1105.66023 ## rm 15.393403 6888.75872 ## age 5.439751 735.33618 ## dis 5.249046 855.71969 ## rad 2.117459 163.24660 ## tax 4.123422 684.25752 ## ptratio 2.589235 963.42017 ## black 4.557716 370.96316 ## lstat 10.676720 5155.26867 ?importance varImpPlot(rf.boston) Plotting test error (MSE) for all different mtry() test.err = double(13) #sequence of all zeros length 13 for(mtry_t in 1:13){ fit = randomForest(medv ~ ., data = data_train, mtry = mtry_t, ntree = 100) pred = predict(fit, data_test) test.err[mtry_t] = mean((pred - data_test$medv)^2) } plot(test.err, pch = 20) lines(test.err) 10.3 Boosting Use the Generalized Boosted Regression Modelling (GBM) Package library(gbm). library(gbm) For boosted regression trees, use distribution = \"gaussian\" but for binary classification problems, use distribution = “bernoulli”. set.seed(517) boost.boston = gbm(medv ~ ., data = data_train, distribution=&quot;gaussian&quot;, n.trees = 1000, interaction.depth = 2) #interaction depth is d in lecture notes summary(boost.boston) #makes a plot too ## var rel.inf ## rm rm 42.6691313 ## lstat lstat 31.2820748 ## crim crim 5.0703499 ## dis dis 4.5616872 ## nox nox 3.2604137 ## age age 3.1196043 ## black black 2.9887370 ## ptratio ptratio 2.6983302 ## tax tax 1.9729225 ## indus indus 1.1820825 ## rad rad 0.8957232 ## chas chas 0.1816023 ## zn zn 0.1173412 pred.boost = predict(boost.boston, newdata = data_test, n.trees = 1000) mean((pred.boost-data_test$medv)^2) #MSE - improvement on random forests ## [1] 16.98637 "],["misc-exam-notes.html", "Chapter 11 Misc Exam Notes 11.1 Plotting 11.2 Generating Synthetic Data", " Chapter 11 Misc Exam Notes 11.1 Plotting Basic Plot plot(x, y, col = &quot;darkgrey&quot;, xlab = &quot;X Label&quot;, ylab = &quot;Y Label&quot;, main = &quot;Title&quot;, bty = &#39;l&#39;, pch = 16) Plotting with a grid Use instead of sort(x) grid = seq(min(x), max(x), length.out = 250) 11.2 Generating Synthetic Data Use rnorm, runif etc functions to generate random data. Trees Example - Surrogate Data set.seed(212) data_surrogate &lt;- data.frame(x = c(runif(200, 0, 0.4), runif(400, 0.4, 0.65), runif(300, 0.65, 1)), y = c(rnorm(200, 1.5), rnorm(400, 0), rnorm(300, 2))) plot(x=data_surrogate$x[1:200], y=data_surrogate$y[1:200], col=&#39;blue&#39;, xlab=&quot;x&quot;, ylab=&quot;y&quot;, pch=16, xlim=c(0,1),ylim=c(-2,4), bty = &#39;l&#39;, main = &quot;Synthetic Surrogate Data&quot;) points(x=data_surrogate$x[201:600], y=data_surrogate$y[201:600], col=&#39;red&#39;, pch=19) points(x=data_surrogate$x[601:900], y=data_surrogate$y[601:900], col=&#39;green&#39;, pch=19) "],["dssc---data-wrangling-presentation-and-applications.html", "Chapter 12 DSSC - Data Wrangling, Presentation and Applications 12.1 Data Wrangling with Tidyverse 12.2 Dynamic Documents and Interactive Dashboards 12.3 Dates 12.4 Strings and Regular Expressions 12.5 Probability Distributions", " Chapter 12 DSSC - Data Wrangling, Presentation and Applications To check for missing data use, print(paste(&quot;Missing data:&quot;, sum(is.na(df$var)), sep=&quot; &quot;, collapse=&quot;&quot;)) 12.1 Data Wrangling with Tidyverse Loading tidyverse, library(&quot;tidyverse&quot;) 12.1.1 Tidy Form (tidyr) What is tidy data? each variable is in a column each observation is in a row each type of observational unit forms a table Moving to and from tidy data Problems (how data may violate tidy form) Data is too wide - one variable spread over multiple columns (use pivot_longer()) Data is too long - one observation spread along multiple rows (use pivot_wider()) pivot_longer() Makes Wide Data Longer The arguments are: Data Frame Columns to transform Name of the column where previous column names should go Name of the column where values from the column should go Example who_wide ## country y1999 y2000 ## 1 Afghanistan 745 2666 ## 2 Brazil 37737 80488 ## 3 China 212258 213766 pivot_longer(who_wide, c(`y1999`, `y2000`), names_to = &quot;year&quot;, values_to = &quot;cases&quot;) ## # A tibble: 6 × 3 ## country year cases ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 Afghanistan y1999 745 ## 2 Afghanistan y2000 2666 ## 3 Brazil y1999 37737 ## 4 Brazil y2000 80488 ## 5 China y1999 212258 ## 6 China y2000 213766 pivot_wider() Makes Long Data Wider The arguments are: Data Frame Columns to transform Name of the column where column names should come from Name of the column where values should come from Example who_long ## country year type count ## 1 Afghanistan 1999 cases 745 ## 2 Afghanistan 1999 population 19987071 ## 3 Afghanistan 2000 cases 2666 ## 4 Afghanistan 2000 population 20595360 ## 5 Brazil 1999 cases 37737 ## 6 Brazil 1999 population 172006362 ## 7 Brazil 2000 cases 80488 ## 8 Brazil 2000 population 174504898 ## 9 China 1999 cases 212258 ## 10 China 1999 population 1272915272 ## 11 China 2000 cases 213766 ## 12 China 2000 population 1280428583 pivot_wider(who_long, names_from = &quot;type&quot;, values_from = &quot;count&quot;) ## # A tibble: 6 × 4 ## country year cases population ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Afghanistan 1999 745 19987071 ## 2 Afghanistan 2000 2666 20595360 ## 3 Brazil 1999 37737 172006362 ## 4 Brazil 2000 80488 174504898 ## 5 China 1999 212258 1272915272 ## 6 China 2000 213766 1280428583 Additional Example - DSSC Lab 5.6 pres.res ## Candidate California Arkansas ## 1 Clinton 8753788/14181595 380494/1130676 ## 2 Trump 4483810/14181595 684872/1130676 ## 3 Other 943997/14181595 65310/1130676 pres.res2 &lt;- pivot_longer(pres.res, c(&quot;California&quot;, &quot;Arkansas&quot;), names_to = &quot;State&quot;, values_to = &quot;Proportion&quot;) pres.res2 ## # A tibble: 6 × 3 ## Candidate State Proportion ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Clinton California 8753788/14181595 ## 2 Clinton Arkansas 380494/1130676 ## 3 Trump California 4483810/14181595 ## 4 Trump Arkansas 684872/1130676 ## 5 Other California 943997/14181595 ## 6 Other Arkansas 65310/1130676 pres.res3 &lt;- separate(pres.res2, &quot;Proportion&quot;, c(&quot;Votes&quot;, &quot;Total&quot;)) pres.res3 ## # A tibble: 6 × 4 ## Candidate State Votes Total ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Clinton California 8753788 14181595 ## 2 Clinton Arkansas 380494 1130676 ## 3 Trump California 4483810 14181595 ## 4 Trump Arkansas 684872 1130676 ## 5 Other California 943997 14181595 ## 6 Other Arkansas 65310 1130676 pres.res4 &lt;- mutate(pres.res3, Votes = as.numeric(Votes), Total = as.numeric(Total)) str(pres.res4) ## tibble [6 × 4] (S3: tbl_df/tbl/data.frame) ## $ Candidate: chr [1:6] &quot;Clinton&quot; &quot;Clinton&quot; &quot;Trump&quot; &quot;Trump&quot; ... ## $ State : chr [1:6] &quot;California&quot; &quot;Arkansas&quot; &quot;California&quot; &quot;Arkansas&quot; ... ## $ Votes : num [1:6] 8753788 380494 4483810 684872 943997 ... ## $ Total : num [1:6] 14181595 1130676 14181595 1130676 14181595 ... pres.res5 &lt;- pres.res4 |&gt; group_by(Candidate) |&gt; summarise(Percent = sum(Votes)/sum(Total)*100) |&gt; arrange(desc(Percent)) pres.res5 ## # A tibble: 3 × 2 ## Candidate Percent ## &lt;chr&gt; &lt;dbl&gt; ## 1 Clinton 59.7 ## 2 Trump 33.8 ## 3 Other 6.59 Other useful tidyr functions separate() - splits one column of strings into multiple new columns unite() - combines many columns into one (as a string) extract() - uses regular expressions to pull out specific information from a string column Example fball ## home away score ## 1 Man U Shef Wed 2-1 ## 2 Tottenham Arsenal 0-0 ## 3 Chelsea W Ham 1-0 separate(fball, &quot;score&quot;, c(&quot;home_goals&quot;, &quot;away_goals&quot;)) ## home away home_goals away_goals ## 1 Man U Shef Wed 2 1 ## 2 Tottenham Arsenal 0 0 ## 3 Chelsea W Ham 1 0 12.1.2 Data Manipulation (dplyr) Main dplyr functions (First argument is always the data frame) filter() - Focus on a subset of rows Other Arguments condition to filter by For example, filter(who, year == 1999) (see above list of logical operators) arrange() - Reorder the rows Other Arguments Variable names to sort by, sub-sorting by later variables Wrap variable name in desc() to sort descending (ascending by default) For example, arrange(who, year, desc(country)) select() - Focus on a subset of variables (columns) Other Arguments Name of variables to retain For example, select(who, year, cases) mutate() - Create new derived variables Other Arguments Name of new variable and equation defining it For example, mutate(who, rate = cases/population) group_by() - Splits a data frame up into groups according to one variable Other Arguments Name of variable to group by For example, group_by(who, country) summarise() - Create summary statistics (collapsing many rows) by groupings Other Arguments Function to summarise by For example, summarise(who, total = sum(cases)) Note: often want to summarise by group For example, who2 &lt;- group_by(who, country) summarise(who2, total = sum(cases), change = max(cases)-min(cases)) 12.1.3 Pipelines Chain functions (not limited to tidyverse functions) where result of first function is first entry in second function and so on. Example, filter(x, ...) |&gt; select(...) |&gt; mutate(...) |&gt; group_by(...) |&gt; arrange(...) Pipeline Operator: CMD-SHIFT-M 12.1.4 Joining Data Frames in Tidyverse Simplest case of joining data frames (more details in data frames section): rbind() - paste rows together (above/below) cbind() - paste cols together (left/right) These methods can be very error prone (requires variables/observations in identical order etc) Advanced Data Frame Joins left_join(x, y) - add new variables from y to x, keeping all x obs right_join(x, y) - add new variables from x to y, keeping all y obs inner_join(x, y) - keep only matching rows full_join(x, y) - keep all rows in both x and y Example band_members ## # A tibble: 3 × 2 ## name band ## &lt;chr&gt; &lt;chr&gt; ## 1 Mick Stones ## 2 John Beatles ## 3 Paul Beatles band_instruments2 ## # A tibble: 3 × 2 ## artist plays ## &lt;chr&gt; &lt;chr&gt; ## 1 John guitar ## 2 Paul bass ## 3 Keith guitar left_join(band_members, band_instruments2, by = c(&quot;name&quot; = &quot;artist&quot;)) ## # A tibble: 3 × 3 ## name band plays ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Mick Stones &lt;NA&gt; ## 2 John Beatles guitar ## 3 Paul Beatles bass 12.2 Dynamic Documents and Interactive Dashboards 12.2.1 RMD Document Preamble --- title: &quot;Example&quot; author: &quot;(optional) Jamie Reason&quot; date: &quot;(optional)&quot; output: html_document: default pdf_document: default --- Course Slides on RMD For further formatting, refer to RMD Cheat Sheet 12.2.2 Shiny Resource: Mastering Shiny Book Outline UI Server R code can be added to any part of a shiny document but only the code in the server will be updated when needed. Starting a Shiny Dashboard (create a new shiny app in R studio): fluidpage() is just the most common but there are alternatives library(shiny) #misc code ui &lt;- fluidpage( ... ) server &lt;- function(input, output, session){ #server code } shinyApp(ui, server) 12.2.2.1 UI UI elements reference guide 12.2.2.1.1 Pages Examples ui &lt;- fluidPage( &quot;One&quot;, &quot;Two&quot;, &quot;Three&quot; ) shinyApp(ui, server = function(input, output, session) {}) ui &lt;- navbarPage( &quot;Title of page&quot;, tabPanel(&quot;My first tab&quot;, &quot;Hello Alice&quot;), tabPanel(&quot;My second tab&quot;, &quot;Hello Bob&quot;) ) shinyApp(ui, server = function(input, output, session) {}) Other pages: fixedPage(), fillPage(), … 12.2.2.1.2 Layouts and Panels Goes inside of the page titlePanel(\"My App\") sidebarLayout() first argument sidebarPanel() second argument mainPanel() fluidrow() - creates a new row with columns in ```column() calls first a number 1 to 12 (all columns numbers must sum to 12) for width -other arguments are outputs Examples ui &lt;- fluidPage( titlePanel(&quot;My App&quot;), sidebarLayout( sidebarPanel(&quot;I&#39;m in sidebar&quot;), mainPanel(&quot;I&#39;m in main panel&quot;) ) ) shinyApp(ui, server = function(input, output, session) {}) ui &lt;- fluidPage( fluidRow( column(4, &quot;Lorem ipsum dolor ...&quot;), column(8, &quot;Lorem ipsum dolor ...&quot;) ), fluidRow( column(6, &quot;Lorem ipsum dolor ...&quot;), column(6, &quot;Lorem ipsum dolor ...&quot;) ) ) shinyApp(ui, server = function(input, output, session) {}) 12.2.2.1.3 UI Inputs All inputs take same first argument - inputId, the unique identifier of the input. This can be accessed by using input$name (in the server). The second argument is a label, or how it’s name appears on the dashboard. Text Inputs textInput() passwordInput() textAreaInput() Numeric Inputs numericInput() sliderInput() Categoric Inputs selectInput() radioButtons() checkboxGroupInput() Examples ui &lt;- fluidPage( numericInput(&quot;num&quot;, &quot;Number one&quot;, value = 0, min = 0, max = 100), sliderInput(&quot;num2&quot;, &quot;Number two&quot;, value = 50, min = 0, max = 100), sliderInput(&quot;rng&quot;, &quot;Range&quot;, value = c(10, 20), min = 0, max = 100) ) shinyApp(ui, server = function(input, output, session) {}) animals &lt;- c(&quot;dog&quot;, &quot;cat&quot;, &quot;mouse&quot;, &quot;bird&quot;, &quot;other&quot;, &quot;I hate animals&quot;) ui &lt;- fluidPage( selectInput(&quot;state&quot;, &quot;What&#39;s your favourite state?&quot;, state.name), radioButtons(&quot;animal&quot;, &quot;What&#39;s your favourite animal?&quot;, animals), checkboxGroupInput(&quot;animal2&quot;, &quot;What animals do you like?&quot;, animals) ) shinyApp(ui, server = function(input, output, session) {}) 12.2.2.2 Server and UI Outputs All outputs take same first argument, outputId and an output can be called by output$name. 12.2.2.2.1 UI Outputs Text Outputs textOutput() renderText() verbatimTextOutput() renderPrint() Plot Outputs plotOutput() and renderPlot() width argument res = 96 argument closest to what you see inj RStudio Examples ui &lt;- fluidPage( textInput(&quot;name&quot;, &quot;What&#39;s your name?&quot;), textOutput(&quot;greet&quot;) ) server &lt;- function(input, output, session) { output$greet &lt;- renderText({ if(nchar(input$name) &gt; 0) { return(paste0(&quot;Hello &quot;, input$name)) } else { return(&quot;Hello friend, tell me your name!&quot;) } }) } shinyApp(ui, server) ui &lt;- fluidPage( plotOutput(&quot;myplot&quot;, width = &quot;400px&quot;) ) server &lt;- function(input, output, session) { output$myplot &lt;- renderPlot({ plot(iris$Sepal.Length, iris$Sepal.Width) }, res = 96) } shinyApp(ui, server) 12.2.2.2.2 Variables outside outputs (reactive) Instead of making variables in the server (which you can’t do as they wouldn’t be reactive), you use the reactive({}) call: Inside the server, name &lt;- ... becomes, name &lt;- reactive({ ... }) And when name is used it should be called as name() Examples server &lt;- function(input, output, session) { name &lt;- reactive({ toupper(input$name) }) output$greet &lt;- renderText({ if(nchar(input$name) &gt; 0) { return(paste0(&quot;Hello &quot;, name(), &quot;, here is your plot ...&quot;)) } else { return(&quot;Hello friend, tell me your name!&quot;) } }) output$myplot &lt;- renderPlot({ if(nchar(input$name) &gt; 0) { ggplot(iris, aes_string(x = input$xvar, y = input$yvar)) + geom_point() + labs(title = paste0(name(), &quot;&#39;s plot!&quot;)) } }, res = 96) } 12.2.2.3 Full Example From exercise 5.78 (Lab 8) library(&quot;shiny&quot;) library(&quot;ukpolice&quot;) library(&quot;tidyverse&quot;) library(&quot;leaflet&quot;) nbd &lt;- ukc_neighbourhoods(&quot;durham&quot;) nbd2 &lt;- nbd$id names(nbd2) &lt;- nbd$name # Define UI for application ui &lt;- fluidPage( titlePanel(&quot;UK Police Data&quot;), sidebarLayout( sidebarPanel( selectInput(&quot;nbd&quot;, &quot;Choose Durham Constabulary Neighborhood&quot;, nbd2), textInput(&quot;date&quot;, &quot;Enter the desired year and month in the format YYYY-MM&quot;, value = &quot;2021-09&quot;) ), # Show a plot of the generated distribution mainPanel( plotOutput(&quot;barchart&quot;), leafletOutput(&quot;map&quot;) ) ) ) # Define server logic server &lt;- function(input, output) { # Get boundaries for selected neighbourhood # Wrapped in a reactive because we need this to trigger a # change when the input neighborhood changes bdy &lt;- reactive({ bdy &lt;- ukc_neighbourhood_boundary(&quot;durham&quot;, input$nbd) bdy |&gt; mutate(latitude = as.numeric(latitude), longitude = as.numeric(longitude)) }) # Get crimes for selected neighbourhood # Also wrapped in a reactive because we need this to trigger a # change when the boundary above, or date, changes crimes &lt;- reactive({ bdy2 &lt;- bdy() |&gt; select(lat = latitude, lng = longitude) ukc_crime_poly(bdy2[round(seq(1, nrow(bdy2), length.out = 100)), ], input$date) }) # First do plot output$barchart &lt;- renderPlot({ ggplot(crimes()) + geom_bar(aes(y = category, fill = outcome_status_category)) + labs(y = &quot;Crime&quot;, fill = &quot;Outcome Status&quot;) }, res = 96) # Then do map output$map &lt;- renderLeaflet({ leaflet() |&gt; addTiles() |&gt; addPolygons(lng = bdy()$longitude, lat = bdy()$latitude) |&gt; addCircles(lng = as.numeric(crimes()$longitude), lat = as.numeric(crimes()$latitude), label = crimes()$category, color = &quot;red&quot;) }) } # Run the application 12.3 Dates (see DSSC Lab 9) Use lubridates package library(&quot;lubridate&quot;) ## Warning: package &#39;lubridate&#39; was built under R version 4.1.2 ## Loading required package: timechange ## Warning: package &#39;timechange&#39; was built under R version 4.1.2 ## ## Attaching package: &#39;lubridate&#39; ## The following objects are masked from &#39;package:base&#39;: ## ## date, intersect, setdiff, union lubridate.tidyverse.org ###Creating Dates {-} Current date and time today() ## [1] &quot;2023-01-05&quot; now() ## [1] &quot;2023-01-05 22:06:35 GMT&quot; str(today()) #these are dates not strings ## Date[1:1], format: &quot;2023-01-05&quot; Constructing dates from strings and numbers ymd(&quot;2021-12-02&quot;) ## [1] &quot;2021-12-02&quot; mdy(&quot;December 2nd, 2021&quot;) ## [1] &quot;2021-12-02&quot; ymd(20211202) ## [1] &quot;2021-12-02&quot; ymd_hms(&quot;2021-12-02 12:33:59&quot;) ## [1] &quot;2021-12-02 12:33:59 UTC&quot; Constructing dates and times from individual components make_date(2021, 12, 2) ## [1] &quot;2021-12-02&quot; make_date(&quot;2021&quot;, &quot;12&quot;, &quot;2&quot;) ## [1] &quot;2021-12-02&quot; make_datetime(2021, 12, 2, 12) ## [1] &quot;2021-12-02 12:00:00 UTC&quot; make_datetime(2021, 12, 2, 12, 33, 59) ## [1] &quot;2021-12-02 12:33:59 UTC&quot; 12.3.1 Time Zones Date creation functions take an argument tz = \"America/New_York\". now(tz = &quot;America/New_York&quot;) ## [1] &quot;2023-01-05 17:06:35 EST&quot; To see all avaliable zones call OlsonNames() Changing Time Zone #forces change of time zone without changing date/time x &lt;- ymd_hm(&quot;2019-12-02 15:10&quot;) force_tz(x, &quot;America/New_York&quot;) ## [1] &quot;2019-12-02 15:10:00 EST&quot; #converts date/tine to a new time zone with_tz(x, &quot;America/New_York&quot;) ## [1] &quot;2019-12-02 10:10:00 EST&quot; 12.3.2 Extracting From Dates datetime &lt;- today() year(datetime) ## [1] 2023 yday(datetime) ## [1] 5 wday(datetime, week_start = 1) #by default, sunday is first day of week, use this to make it monday ## [1] 4 month(datetime, label = TRUE) ## [1] Jan ## 12 Levels: Jan &lt; Feb &lt; Mar &lt; Apr &lt; May &lt; Jun &lt; Jul &lt; Aug &lt; Sep &lt; ... &lt; Dec Rounding Dates/Times floor_date(datetime, unit = &quot;minute&quot;) ## [1] &quot;2023-01-05 UTC&quot; ceiling_date(datetime, unit = &quot;week&quot;) ## [1] &quot;2023-01-08&quot; ceiling_date(datetime, unit = &quot;quarter&quot;) ## [1] &quot;2023-04-01&quot; floor_date(datetime, unit = &quot;week&quot;, week_start = 1) ## [1] &quot;2023-01-02&quot; 12.3.3 Misc Updating Dates/Times datetime &lt;- ymd_hms(&quot;2021-12-02 12:33:59&quot;) datetime &lt;- update(datetime, hour = 11, second = 33) datetime ## [1] &quot;2021-12-02 11:33:33 UTC&quot; #Alternatively, datetime &lt;- ymd_hms(&quot;2021-12-02 12:33:59&quot;) hour(datetime) &lt;- 11 second(datetime) &lt;- 33 datetime ## [1] &quot;2021-12-02 11:33:33 UTC&quot; Durations Can do arithmetic with dates and times einstein &lt;- dmy(&quot;14th March 1879&quot;) age &lt;- today() - months(42) - einstein #age 42 months ago age ## Time difference of 51247 days Get a duration after arithmetic using as.duration() as.duration(age) ## [1] &quot;4427740800s (~140.31 years)&quot; 12.4 Strings and Regular Expressions 12.4.1 Strange characters When you want a string with strange characters, enclose it in r\"(...)\" instead of just \"...\". z &lt;- r&quot;(As Roosevelt said, &quot;Believe you can and you&#39;re halfway there.&quot; )&quot; cat(z) ## As Roosevelt said, ## &quot;Believe you can and you&#39;re halfway there.&quot; cat() is like a print command 12.4.2 stringr (part of tidyverse) Most stringr functions begin with str_ so can use autocomplete for many string operations. Basics String Length str_length(c(&quot;Data Science and Statistical Computing&quot;, &quot;by&quot;, &quot;Dr Louis Aslett&quot;)) ## [1] 38 2 15 Combining Strings str_c(&quot;Data Science and Statistical Computing&quot;, &quot;by&quot;, &quot;Dr Louis Aslett&quot;) ## [1] &quot;Data Science and Statistical ComputingbyDr Louis Aslett&quot; str_c(&quot;Data Science and Statistical Computing&quot;, &quot;by&quot;, &quot;Dr Louis Aslett&quot;, sep = &quot; &quot;) ## [1] &quot;Data Science and Statistical Computing by Dr Louis Aslett&quot; str_c(c(&quot;Data Science and Statistical Computing&quot;, &quot;by&quot;, &quot;Dr Louis Aslett&quot;)) ## [1] &quot;Data Science and Statistical Computing&quot; ## [2] &quot;by&quot; ## [3] &quot;Dr Louis Aslett&quot; str_c(c(&quot;Data Science and Statistical Computing&quot;, &quot;by&quot;, &quot;Dr Louis Aslett&quot;), collapse = &quot; &quot;) ## [1] &quot;Data Science and Statistical Computing by Dr Louis Aslett&quot; Subsetting Strings z &lt;- c(&quot;Alice&quot;, &quot;Bob&quot;, &quot;Connie&quot;, &quot;David&quot;) str_sub(z, 1, 4) ## [1] &quot;Alic&quot; &quot;Bob&quot; &quot;Conn&quot; &quot;Davi&quot; str_sub(z, 1, 2) &lt;- &quot;Zo&quot; z ## [1] &quot;Zoice&quot; &quot;Zob&quot; &quot;Zonnie&quot; &quot;Zovid&quot; Trimming str_trim(&quot; String with trailing, middle, and leading white space\\n\\n&quot;) ## [1] &quot;String with trailing, middle, and leading white space&quot; str_squish(&quot; String with trailing, middle, and leading white space\\n\\n&quot;) ## [1] &quot;String with trailing, middle, and leading white space&quot; 12.4.2.1 Regex’s See all details in docs or lecture slides Regex’s are used for finding patterns in strings str_view() Identify a pattern in a string: Exact matching str_view(&quot;string to find pattern in&quot;, &quot;pattern&quot;) ## [1] │ string to find &lt;pattern&gt; in Wildcard matching x &lt;- c(&quot;apple&quot;, &quot;banana&quot;, &quot;pear&quot;) str_view(x, &quot;.a.&quot;) ## [2] │ &lt;ban&gt;ana ## [3] │ p&lt;ear&gt; How to match a .? - str_view(c(\".bc\", \"a.c\", \"be.\"), \"a\\\\.c\") (use \\ but make sure to escape it) Anchoring To start: str_view(x, &quot;^a&quot;) ## [1] │ &lt;a&gt;pple To end: str_view(x, &quot;a$&quot;) ## [2] │ banan&lt;a&gt; can also anchor to both. Matching Set of Characters I Find exactly first character that matches: str_view(x, &quot;[pan]&quot;) ## [1] │ &lt;a&gt;&lt;p&gt;&lt;p&gt;le ## [2] │ b&lt;a&gt;&lt;n&gt;&lt;a&gt;&lt;n&gt;&lt;a&gt; ## [3] │ &lt;p&gt;e&lt;a&gt;r Find one or more instance consecutively: str_view(x, &quot;[pan]+&quot;) ## [1] │ &lt;app&gt;le ## [2] │ b&lt;anana&gt; ## [3] │ &lt;p&gt;e&lt;a&gt;r Find exact number of instances occurring consecutively: str_view(x, &quot;[pan]{2}&quot;) ## [1] │ &lt;ap&gt;ple ## [2] │ b&lt;an&gt;&lt;an&gt;a Find a range or instances occurring consecutively: str_view(x, &quot;[pan]{1,3}&quot;) ## [1] │ &lt;app&gt;le ## [2] │ b&lt;ana&gt;&lt;na&gt; ## [3] │ &lt;p&gt;e&lt;a&gt;r Matching Set of Characters II y &lt;- c(&quot;There were 122 in total&quot;, &quot;Overall about 390 found&quot;, &quot;100 but no more&quot;) str_view(y, &quot;[0-9]+&quot;) ## [1] │ There were &lt;122&gt; in total ## [2] │ Overall about &lt;390&gt; found ## [3] │ &lt;100&gt; but no more str_view(y, &quot;[^A-Za-z ]+&quot;) #^ anchor inside so acts as a negation ## [1] │ There were &lt;122&gt; in total ## [2] │ Overall about &lt;390&gt; found ## [3] │ &lt;100&gt; but no more str_view(y, &quot;^[0-9]+&quot;) #^ anchor on outside ## [3] │ &lt;100&gt; but no more str_view(y, &quot;[a-z ]+&quot;) ## [1] │ T&lt;here were &gt;122&lt; in total&gt; ## [2] │ O&lt;verall about &gt;390&lt; found&gt; ## [3] │ 100&lt; but no more&gt; 12.5 Probability Distributions Letter Function Use “d” dnorm() evaluates pdf \\(f(x)\\) “p” pnorm() evaluates cdf \\(F(x)\\) “q” qnorm() evaluates inverse cdf \\(F^{-1}(q)\\) i.e. \\(P(X \\leq x) = q\\) “r” rnorm() generates random numbers Parameters will vary, e.g. Normal distribution: dnorm, pnorm, qnorm, rnorm. Parameters: mean (\\(\\mu\\)) and sd (\\(\\sigma\\)). t distribution: dt, pt, qt, rt. Parameter: df \\(\\chi^2\\) distribution: dchisq, pchisq, qchisq, rchisq. Parameter: df 12.5.1 DSSC Theory Applications 12.5.1.1 Monte Carlo Hyothesis Test Example 2.1 # Specify test statistic and null value x.bar &lt;- 8.6 n &lt;- 6 mu0 &lt;- 9.2 # Simulate lots of data assuming the null is true t &lt;- rep(0, 50000) for(j in 0:50000) { z &lt;- rnorm(n, mu0, sqrt(0.4)) #random sample (of n=6) generated under H0 t[j] &lt;- abs(mean(z)-mu0) #difference in mean of random sample and mean under H0 assumption } # Calculate empirical p-value sum(t &gt; abs(x.bar-mu0)) / 50000 #number of random samplea that were at least as far from mu0 as observation 12.5.1.2 Boot Strap Set-up Sample of size \\(n\\) independent samples There is a statistic \\(S( \\cdot )\\) we wish to estimate We also want the standard error of this General Method: Draw \\(B\\) new samples of size \\(n\\) with replacement from \\(\\mathbf{x} = (x_1, \\ldots , x_n)\\) Call these samples \\(\\textbf{x}^{\\star 1}, \\ldots , \\textbf{x}^{\\star B}\\) Calculate the estimate, \\(\\bar{S}^{\\star}=\\frac{1}{B} \\sum_{b=1}^{B} S\\left(\\mathbf{x}^{\\star b}\\right)\\) Calculate the variance, \\(\\widehat{\\operatorname{Var}}(S(\\mathbf{x}))=\\frac{1}{B-1} \\sum_{b=1}^{B}\\left(S\\left(\\mathbf{x}^{\\star b}\\right)-\\bar{S}^{\\star}\\right)^{2}\\) Example 3.1 (Also see 3.5) # Mouse data x &lt;- c(94,197,16,38,99,141,23) # Number of bootstraps B &lt;- 1000 # Statistic S &lt;- mean # Perform bootstrap S.star &lt;- rep(0, B) for(b in 1:B) { x.star &lt;- sample(x, replace = TRUE) S.star[b] &lt;- S(x.star) } # Bootstrap estimate mean(S.star) # Standard error of estimate sd(S.star) Empirical CDF - ecdf(x) "]]
