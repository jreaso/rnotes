# Multiple Linear Regression

## Exploring Data With Multiple Predictor Variables

### Pairs Plot (Scatterplot Matrix)
To see how any pairs of variable in a data are related use `pairs(data)`. For an example with `carSales` data from [Section 3.6](https://bookdown.org/hailiangdu80/Machine_Learning_and_Neural_Networks/linear-regression.html#regression-in-r-1), 

```{r echo=TRUE, include=FALSE}
carSales <- data.frame(Price = c(85, 103,  70,  82,  89,  98,  66,  95, 169,  70,  48),
                     Age = c(5, 4, 6, 5, 5, 5, 6, 6, 2, 7, 7),
                     Miles = c(57, 40, 77, 60, 49, 47, 58, 39, 8, 69, 89))
```

```{r}
#can make it symmetric on diagonal by leaving as `pairs(carSales)`
pairs(carSales, lower.panel = NULL)
```

The upper panel here can be customised by making a new function

**Example from Section 3.6**

```{r}
# Customize upper panel
upper.panel <- function(x, y){
  points(x,y, pch=19, col=4)
  r <- round(cor(x, y), digits=3)
  txt <- paste0("r = ", r)
  usr <- par("usr"); on.exit(par(usr))
  par(usr = c(0, 1, 0, 1))
  text(0.5, 0.9, txt)
}
pairs(carSales, lower.panel = NULL, 
      upper.panel = upper.panel)
```

**Example from Practical 2**

```{r include=FALSE}
library("faraway")
fat1 <- fat[,-c(2,3,8)] 
```

```{r}
panel.cor <- function(x, y){
  usr <- par("usr"); on.exit(par(usr))
  par(usr = c(0, 1, 0, 1))
  r <- round(cor(x, y), digits=2)
  txt <- paste0(" ", r)
  text(0.5, 0.5, txt, cex = 0.8)
}
# Customize upper panel
upper.panel<-function(x, y){
  points(x,y, pch = 19)
}
# Create the plots
pairs(fat1, 
      lower.panel = panel.cor,
      upper.panel = upper.panel)
```

### Using `corrplot` Package
```{r message=FALSE}
library(corrplot)
```

**Examples from Practical 2**

```{r}
corrplot(cor(fat1), method = "number", type = "upper", diag = FALSE)
corrplot.mixed(cor(fat1), upper = "ellipse", lower = "number",number.cex = .7)
```

## Basics of Building an MLR Model (Including Transformations)

**Build a Model With Specific Predictors**

```{r eval=FALSE}
model <- lm(Y ~ X1 + X2 + ... + Xn, data=data)
```

**Build a Model With All Predictors**

```{r eval=FALSE}
model <- lm(Y ~ ., data=data)
```

**Build a Model With Specific Predictors Removed**

```{r eval=FALSE}
model <- lm(Y ~ . - X1 - X2, data=data)
```

**Example Including Transformations**

```{r eval=FALSE}
model <- lm(Y ~ X1 + X2*X3 + log(X4) + I(X5^2), data=data)
```

Note that `X2*X3` includes the predictors `X2`, `X3` and their product `X2 x X3.`

Inference is much the same as with SLR.


## Multicollinearity Detection and VIF

To calculate VIF value for each predictor in a model simply use `vif()` function from `car library`.

```{r include=FALSE}
reg <- lm(Price ~ Age + Miles, data=carSales)
```

```{r message=FALSE, warning=FALSE}
library(car)
vif(reg)
```

## Model Selection

The following subsection follows closely to [Practical Demonstration 5.4](https://bookdown.org/hailiangdu80/Machine_Learning_and_Neural_Networks/model-search-methods.html#prac-model-search-methods). Most examples will be from there.

```{r include=FALSE}
library(ISLR)
Hitters <- na.omit(Hitters)
```

### Best Subset and Forward/Backward Stepwise Selection (Using Selection Criteria)

Best Subset Selection is done using `regsubsets()` function from `leaps` library.

```{r message=FALSE, warning=FALSE}
library(leaps)
```

```{r}
#function for best subset selection
best = regsubsets(Salary ~ ., Hitters, nvmax=19)
results = summary(best)

names(results) #gives the names of the predictors chosen by best subset selection
```

`nvmax=19` specifies that you want all 19 models and not just up to the best one.

**Data you can extract from output of `regsubsets()`**

```{r}
RSS = results$rss
r2 = results$rsq
Cp = results$cp
BIC = results$bic
Adj_r2 = results$adjr2

cbind(RSS, r2, Cp, BIC, Adj_r2)
```

#### Plotting To Understand Best Subset Selection {-}

```{r}
#Plots RSS and R-square as you add more predictors
par(mfrow = c(1, 2))
plot(RSS, xlab = "Number of Predictors", ylab = "RSS", 
     type = "l", lwd = 2)
plot(r2, xlab = "Number of Predictors", ylab = "R-square", 
     type = "l", lwd = 2)
```

**Number of predictors in optimal model under different selection criteria**

```{r}
which.min(Cp)
which.min(BIC)
which.max(Adj_r2)
```

**Example Plot For How Selection Criteria Change**

```{r}
par(mfrow = c(1, 3))
plot(Cp, xlab = "Number of Predictors", ylab = "Cp", 
     type = 'l', lwd = 2)
points(10, Cp[10], col = "red", cex = 2, pch = 8, lwd = 2)
plot(BIC, xlab = "Number of Predictors", ylab = "BIC", 
     type = 'l', lwd = 2)
points(6, BIC[6], col = "red", cex = 2, pch = 8, lwd = 2)
plot(Adj_r2, xlab = "Number of Predictors", ylab = "Adjusted RSq", 
     type = "l", lwd = 2)
points(11, Adj_r2[11],  col = "red", cex = 2, pch = 8, lwd = 2)
```

**Built-in Plot for Criteria**

Top row is the best model under the criteria and worst is at bottom

```{r}
plot(best, scale = "Cp")
```

The options for `scale` are `"bic"`, `"Cp"`, `"adjr2"` and `"r2"`.

**Extracting Coefficients for Best Model** (at a specific number of predictors)

```{r}
coef(best, 10) #Cp
```

The number here which is `k=10` corresponds to model `M_k`. We found this from `which.min(Cp)`.

#### Forward and Backward Stepwise Selection {-}

To use forward or backward stepwise selection instead change the `method` parameter is `regsubsets()` function.

```{r}
fwd = regsubsets(Salary~., data = Hitters, nvmax = 19, method = "forward")
```

### Best Subset Selection (Using Validation)

'regsubsets()' does not have a built-in function to do best subsey selection using validation so we use a custom function (copy and paste).

```{r}
predict.regsubsets = function(object, newdata, id, ...){
  form = as.formula(object$call[[2]])
  mat = model.matrix(form, newdata)
  coefi = coef(object, id = id)
  xvars = names(coefi)
  mat[, xvars]%*%coefi
}
```
- `object` should be the result of a call to `regsubsets`
- `newdata` should be data frame with data from validation set in
- `id` specifies we want to use the model from `object` with `id` predictors

#### Splitting Data into Training and Validation {-}

As a rough guide, split the data approximately 2:1 for Training:Validation.

```{r results='hide'}
dim(Hitters)
training.obs = sample(1:263, 175)
Hitters.train = Hitters[training.obs,  ]
Hitters.test = Hitters[-training.obs,  ]
```

#### Best Subset Validation {-}

The `regsubsets` call:
```{r}
best.val = regsubsets(Salary ~ ., data = Hitters.train, nvmax = 19)
```

Iterate through every number of predictors and calculate the MSE between prediction and observed (from validation set).
```{r}
val.error <- c()
for(i in 1:19){
  pred = predict.regsubsets(best.val, Hitters.test, i)
  val.error[i] = mean((Hitters.test$Salary - pred)^2) #MSE
}

val.error #stores 19 models validation MSE
```

Index with smallest MSE is best number of predictors

```{r}
which.min(val.error) 
```

#### Inference {-}

After choosing optimal model with best subset selection using a validation set, **before inference, you must train the model in the entire data set**.

```{r}
coef(best.val, 10) # Check which variables to use in the lm.
ls10 = lm(Salary ~ AtBat + Hits + Runs + Walks + CAtBat + CRuns + CRBI + CWalks + Division + PutOuts, data = Hitters)
```

Usual inference can follow.

Note: using a validation set means the choice of the best model is random, depending on how you selected which observations were used for training and which were used for validation.

### Best Subset Selection (Using Cross-Validation)

Doing cross-validation is generally preferable to regular validation as it utilises entire data set for training and testing.

Continuing with `Hitters` data from [Example 5.4](https://bookdown.org/hailiangdu80/Machine_Learning_and_Neural_Networks/model-search-methods.html#prac-cross-val),

```{r}
best = regsubsets(Salary ~ ., data = Hitters, nvmax = 19)
```

This method requires manually specifying the models you include in the cross-validation. So, for this example, we use cross-validation to compare the three models suggested by $C_p$, BIC and adjusted $R^2$. (Recall: could use `coef(best,10)` to find coefficients in $C_p$ model since $C_p$ found 10 predictors was best).

#### Train Models on Entire Data Set {-}

```{r}

ls10 = lm(Salary ~ AtBat + Hits + Walks + CAtBat + CRuns + CRBI + CWalks +
            Division + PutOuts + Assists, data = Hitters)
ls6 = lm(Salary ~ AtBat + Hits + Walks + CRBI + Division + PutOuts, 
         data = Hitters)
ls11 = lm(Salary ~ AtBat + Hits + Walks + CAtBat + CRuns + CRBI + CWalks +
            + League + Division + PutOuts + Assists, data = Hitters)
```

#### Creating The Folds {-}

Using $k = 10$ folds,
```{r}
k = 10
folds = cut(1:263, breaks=10, labels=FALSE)

table(folds)
```

However, currently the folds are not random, the first fold is just the first 27 entries. We can randomly shuffle the folds using `sample`.

```{r}
set.seed(2)
folds = sample(folds)
folds
```


#### Calculating The CV Errors {-}

We first create a matrix to store the CV errors,

```{r}
cv.errors = matrix(NA, nrow = k, ncol = 3, 
                   dimnames = list(NULL, c("ls10", "ls6", "ls11")))
cv.errors 
```


Then, iterating through each fold, we obtain the CV errors,

```{r}
for(i in 1:k){
  #Trains models on all folds but i
  ls10 = lm(Salary ~ AtBat + Hits + Walks + CAtBat + CRuns + CRBI + CWalks +
              Division + PutOuts + Assists, data = Hitters[folds!=i, ] )
  ls6 = lm(Salary ~ AtBat + Hits + Walks + CRBI + Division + PutOuts, 
           data = Hitters[folds!=i, ])
  ls11 = lm(Salary ~ AtBat + Hits + Walks + CAtBat + CRuns + CRBI + CWalks +
              + League + Division + PutOuts + Assists, 
            data = Hitters[folds!=i, ])
  
  #Predicts values in fold i
  pred10 <- predict( ls10, newdata = Hitters[folds==i, ] )
  pred6 <- predict( ls6, newdata = Hitters[folds==i, ] )
  pred11 <- predict( ls11, newdata = Hitters[folds==i, ] )
  
  #Calculates CV errors
  cv.errors[i,] = c( mean( (Hitters$Salary[folds==i]-pred10)^2), #calculate MSE for cv
                     mean( (Hitters$Salary[folds==i]-pred6)^2), 
                     mean( (Hitters$Salary[folds==i]-pred11)^2) )
}
cv.errors
```

Calculating the means gives a performance metric for each model.

```{r}
cv.mean.errors <- colMeans(cv.errors)
cv.mean.errors
```