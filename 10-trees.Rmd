# Tree Models

```{r echo=FALSE}
#Synthetic Surrogate Data
set.seed(212)
data_surrogate <- data.frame(x = c(runif(200, 0, 0.4), runif(400, 0.4, 0.65), runif(300, 0.65, 1)),
                             y = c(rnorm(200, 1.5), rnorm(400, 0), rnorm(300, 2)))
```

## Building Tree Models

```{r warning=FALSE}
library("tree")
```

Use `tree()` function from `library("tree")`.

```{r}
tree_fit = tree(y ~ x, data_surrogate)
summary(tree_fit)
#tree_fit
```

`summary(tree_fit)` includes a lot of useful information such as the *number of terminal nodes* and the *training misclassification error rate*.

### Plotting Trees {-}

If `pretty = 0` then the level names of a factor split attributes are used unchanged.

```{r}
plot(tree_fit)
text(tree_fit, pretty=0)
```


### Predictions {-}

Example from practical demonstration with synthetic surrogate data.

```{r}
set.seed(347)
data_surrogate_test <- data.frame(x = c(runif(200, 0, 0.4), runif(400, 0.4, 0.65), runif(300, 0.65, 1)),
                                  y = c(rnorm(200, 1.5),    rnorm(400, 0),         rnorm(300, 2)))
```

Use `predict()` function. The tree model gives the mean in that terminal node group as a prediction.

```{r}
tree_pred = predict(tree_fit, data_surrogate_test)
```

When you are predicting a factor variable, use `type = "class"` to specify a prediction from the class.

**Plotting Predictions**
```{r}
plot(x = data_surrogate_test$x[1:200], y = data_surrogate_test$y[1:200],
     col = 'blue', xlab = "X", ylab = "Y", pch = 19, xlim = c(0,1), ylim = c(-2,4))
points(x = data_surrogate_test$x[201:600], y = data_surrogate_test$y[201:600], col = 'red', pch = 19)
points(x = data_surrogate_test$x[601:900], y = data_surrogate_test$y[601:900], col = 'green', pch = 19)

points(x=data_surrogate_test$x,y=tree_pred,col='black',pch=15)
```

**MSE**
```{r}
pred_mse=mean((data_surrogate_test$y-tree_pred)^2)
pred_mse
```

**Calculating Prediction Classification Error (Classification Trees)**

Example from mock practical exam:

```{r echo=FALSE}
library(tree)
library(ISLR)
attach(Carseats)

High=ifelse(Sales<=8,"No","Yes")
Carseats=data.frame(Carseats, High)
Carseats$High <- as.factor(Carseats$High)

set.seed(743)
train_index=sample(1:nrow(Carseats),250)
data_train=Carseats[train_index,]
data_test=Carseats[-train_index,]
```
```{r}
tree.carseats = tree(High ~ .-Sales, data_train)

tree.pred = predict(tree.carseats, data_test, type="class")
table(tree.pred, data_test$High)
(80+31)/150
```

The percentage of correct predictions is 74%.

### Pruning Trees {-}

```{r echo=FALSE}
set.seed(739)
data_surrogate_s <- data.frame(x = c(runif(20, 0, 0.4), runif(40, 0.4, 0.65), runif(30, 0.65, 1)),
                               y = c(rnorm(20, 1.5),    rnorm(40, 0),         rnorm(30, 2)))
tree_fit_s = tree(y~x,data_surrogate_s)
tree_pred_s = predict(tree_fit_s,data_surrogate_test)
pred_mse=mean((data_surrogate_test$y-tree_pred_s)^2)
```


Prune tree with `prune.tree()` command, but can first plot the `deviance` for different size trees using `cv.tree()` with parameter `FUN = prune.tree`. Alternatively, can use parameter `FUN = prune.misclass` to plot misclass (when a classification tree - factor variable).

```{r}
tree_cv_prune = cv.tree(tree_fit_s, FUN = prune.tree)
#tree_cv_prune
plot(tree_cv_prune)
```

The "best" sized tree (via CV) is the one with the lowest deviance/misclass.

**Fitting Pruned Model**

```{r}
tree_fit_prune = prune.tree(tree_fit_s, best = 3)
tree_fit_prune
```

You may also prune the tree by specifying the cost-complexity parameter `k`, for example `tree_fit_prune=prune.tree(tree_fit_s, k=5)`.

**Note for classification trees:** Regression trees must use the default `method = deviance` parameter but for classification trees, this parameter can be `deviance` or `misclass` (`misclass` prefers trees with lower misclassification rate). Shorthand for including `method = misclass` is to use the `prune.misclass` function instead of `prune.tree`.

Pruned Tree:
```{r}
plot(tree_fit_prune)
text(tree_fit_prune,pretty=0)
```

Pruning has decreased the MSE:

```{r}
tree_pred_prune=predict(tree_fit_prune,data_surrogate_test)
pred_mse_prune = mean((data_surrogate_test$y-tree_pred_prune)^2)
pred_mse - pred_mse_prune
```

The following function shows this is a general result and holds on average:

```{r}
prune_improvement <- function(k) {
  data_surrogate_s <- data.frame(x = c(runif(20, 0, 0.4), runif(40, 0.4, 0.65), runif(30, 0.65, 1)),
                                 y = c(rnorm(20, 1.5),    rnorm(40, 0),         rnorm(30, 2)))
  tree_fit_s = tree(y~x,data_surrogate_s)
  tree_pred_s = predict(tree_fit_s,data_surrogate_test)
  pred_mse_s = mean((data_surrogate_test$y-tree_pred_s)^2)
  tree_fit_prune = prune.tree(tree_fit_s,k=5)
  tree_pred_prune = predict(tree_fit_prune,data_surrogate_test)
  pred_mse_prune = mean((data_surrogate_test$y-tree_pred_prune)^2)
  dif_t = pred_mse_s-pred_mse_prune
  return(dif_t)  
}
mean(sapply(1:1024, FUN=function(i){prune_improvement(5)}))
```


## Bagging and Random Forests

Follows an example which uses `library(ISLR)` and `library(MASS)` for bagging, random forests and boosted trees. We have `data_train` and `data_test` from the `Boston` dataset.

```{r echo=FALSE, warning=FALSE}
library(MASS)

set.seed (1)
train = sample (1: nrow(Boston ), nrow(Boston)/2)
data_train = Boston[train,]
data_test = Boston[-train,]
```

Tree Model and Pruned Tree Model Trained:
```{r}
tree.boston = tree(medv ~ ., data_train)
prune.boston = prune.tree(tree.boston, best = 6)
```

### Bagging Model {-}

For bagging, we use the `library(randomForest)` package but with `mrty = 13` ($m = p$ so using all predictors).

```{r echo=FALSE, warning=FALSE, message=FALSE}
library(randomForest)
```

**Bagging Model (10 Trees):**

```{r}
set.seed (472)
bag.boston = randomForest(medv ~ ., data = data_train, mtry=13, ntree=10, importance = TRUE)
```

**Prediction Accuracy:**
```{r}
pred.bag = predict(bag.boston, newdata = data_test)
plot(pred.bag, data_test$medv, bty = 'l')
abline(0,1)
```

**MSE:**
```{r}
mean((pred.bag - data_test$medv)^2)
```


**Same for 100 and 1000 trees:**
```{r}
#100 trees
bag.boston = randomForest(medv ~ ., data = data_train, mtry = 13, ntree = 100, importance = TRUE)
pred.bag = predict(bag.boston, newdata = data_test)
mean((pred.bag - data_test$medv)^2) #MSE

#1000 trees
bag.boston = randomForest(medv ~ ., data = data_train, mtry = 13, ntree = 1000, importance = TRUE)
pred.bag = predict(bag.boston, newdata = data_test)
mean((pred.bag - data_test$medv)^2) #MSE
```

### Random Forests {-}

Use `mtry` as something else for random forests. By default this will be $p/3$ but can also try $\sqrt{p}$. Very similar method to bagging.

```{r}
rf.boston = randomForest(medv ~ ., data = data_train, mtry = 4, ntree = 100, importance = TRUE) #uses 4 variables
pred.rf = predict(rf.boston, newdata = data_test)
mean((pred.rf -data_test$medv)^2) #MSE
```

**Importance Plot**

```{r}
importance(rf.boston)
?importance
varImpPlot(rf.boston)
```

Higher up variables have higher importance (prediction power). MSE and Gini Index usually give similar plots but can differ significantly when there are categorical variables, especially with many levels.

**Plotting test error (MSE) for all different `mtry()`**

```{r}
test.err = double(13) #sequence of all zeros length 13

for(mtry_t in 1:13){
  fit = randomForest(medv ~ ., data = data_train, mtry = mtry_t, ntree = 100)
  pred = predict(fit, data_test)
  test.err[mtry_t] = mean((pred - data_test$medv)^2)
}

plot(test.err, pch = 20)
lines(test.err)
```


## Boosting

Use the Generalized Boosted Regression Modelling (GBM) Package `library(gbm)`.

```{r warning=FALSE, message=FALSE}
library(gbm)
```

For boosted regression trees, use `distribution = "gaussian"` but for binary classification problems, use `distribution = “bernoulli”`.

```{r}
set.seed(517)
boost.boston = gbm(medv ~ ., data = data_train, distribution="gaussian", n.trees = 1000, interaction.depth = 2)
#interaction depth is d in lecture notes
summary(boost.boston) #makes a plot too

pred.boost = predict(boost.boston, newdata = data_test, n.trees = 1000)
mean((pred.boost-data_test$medv)^2) #MSE - improvement on random forests
```

Interaction depth is the number of splits performed on each tree in the boosting model.

If you want to predict a binary classification problem, use `type = "response"` in `predict()` function. The `type = "response"` option tells `R` to output probabilities of the form `P(Y=1|X)` instead of other information such as logit.


**Mock Practical Exam Example**

```{r warning=FALSE}
library(modeldata)
library(randomForest)
library(gbm)
data(credit_data)
credit_data=na.omit(credit_data)

set.seed(180)
train = sample (1: nrow(credit_data), floor(nrow(credit_data )/2))
data_train=credit_data[train,]
data_test=credit_data[-train,]

boost.credit=gbm(unclass(Status)-1~.,data=data_train,distribution="bernoulli",
                 n.trees =1000,interaction.depth=2)
summary(boost.credit)

pred.boost=predict(boost.credit,newdata =data_test, n.trees =1000, type="response")
pred.boost.wrong=predict(boost.credit,newdata =data_test, n.trees =1000)

status_pred=ifelse(pred.boost<=0.5,"bad","good")
class_table=table(status_pred, data_test$Status)
success_rate=(class_table[1,1]+class_table[2,2])/sum(class_table)
success_rate
```





